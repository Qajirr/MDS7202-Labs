{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyPTffTLug7i"
   },
   "source": [
    "# **Laboratorio 11: LLM y Agentes Aut칩nomos 游뱄**\n",
    "\n",
    "MDS7202: Laboratorio de Programaci칩n Cient칤fica para Ciencia de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pbWVyntzbvL"
   },
   "source": [
    "### **Cuerpo Docente:**\n",
    "\n",
    "- Profesores: Ignacio Meza, Sebasti치n Tinoco\n",
    "- Auxiliar: Eduardo Moya\n",
    "- Ayudantes: Nicol치s Ojeda, Melanie Pe침a, Valentina Rojas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dy6ikgVYzghB"
   },
   "source": [
    "### Equipo: **SUPER IMPORTANTE - notebooks sin nombre no ser치n revisados**\n",
    "\n",
    "- Nombre de alumno 1: Joaqu칤n De Groote\n",
    "- Nombre de alumno 2: Vicente Pinochet R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMJ-owchzjFf"
   },
   "source": [
    "### **Link de repositorio de GitHub:** [Insertar Enlace](https://github.com/Qajirr/MDS7202-Labs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUuwsXrKzmkK"
   },
   "source": [
    "## **Temas a tratar**\n",
    "\n",
    "- Reinforcement Learning\n",
    "- Large Language Models\n",
    "\n",
    "## **Reglas:**\n",
    "\n",
    "- **Grupos de 2 personas**\n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser치n respondidos por este medio.\n",
    "- Prohibidas las copias.\n",
    "- Pueden usar cualquer matrial del curso que estimen conveniente.\n",
    "\n",
    "### **Objetivos principales del laboratorio**\n",
    "\n",
    "- Resoluci칩n de problemas secuenciales usando Reinforcement Learning\n",
    "- Habilitar un Chatbot para entregar respuestas 칰tiles usando Large Language Models.\n",
    "\n",
    "El laboratorio deber치 ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m치ximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante m치s eficientes que los iteradores nativos sobre DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hmHHQ9BuyAG"
   },
   "source": [
    "## **1. Reinforcement Learning (2.0 puntos)**\n",
    "\n",
    "En esta secci칩n van a usar m칠todos de RL para resolver dos problemas interesantes: `Blackjack` y `LunarLander`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gOcejYb6uzOO"
   },
   "outputs": [],
   "source": [
    "!pip install -qqq gymnasium stable_baselines3\n",
    "!pip install -qqq swig\n",
    "!pip install -qqq gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBPet_Mq8dX9"
   },
   "source": [
    "### **1.1 Blackjack (1.0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.recreoviral.com/wp-content/uploads/2016/08/s3.amazonaws.com-Math.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "La idea de esta subsecci칩n es que puedan implementar m칠todos de RL y as칤 generar una estrategia para jugar el cl치sico juego Blackjack y de paso puedan ~~hacerse millonarios~~ aprender a resolver problemas mediante RL.\n",
    "\n",
    "Comencemos primero preparando el ambiente. El siguiente bloque de c칩digo transforma las observaciones del ambiente a `np.array`:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</p>\n",
    "\n",
    "La idea de esta subsecci칩n es que puedan implementar m칠todos de RL y as칤 generar una estrategia para jugar el cl치sico juego Blackjack y de paso puedan ~~hacerse millonarios~~ aprender a resolver problemas mediante RL.\n",
    "\n",
    "Comencemos primero preparando el ambiente. El siguiente bloque de c칩digo transforma las observaciones del ambiente a `np.array`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LpZ8bBKk9ZlU"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import MultiDiscrete\n",
    "import numpy as np\n",
    "\n",
    "class FlattenObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(FlattenObservation, self).__init__(env)\n",
    "        self.observation_space = MultiDiscrete(np.array([32, 11, 2]))\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.array(observation).flatten()\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = gym.make(\"Blackjack-v1\")\n",
    "env = FlattenObservation(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJ6J1_-Y9nHO"
   },
   "source": [
    "#### **1.1.1 Descripci칩n de MDP (0.2 puntos)**\n",
    "\n",
    "Entregue una breve descripci칩n sobre el ambiente [Blackjack](https://gymnasium.farama.org/environments/toy_text/blackjack/) y su formulaci칩n en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5i1Wt1p770x"
   },
   "source": [
    "`escriba su respuesta ac치`\n",
    "\n",
    "**Blackjack - Descripci칩n del Ambiente**\n",
    "\n",
    "El objetivo en **Blackjack** es vencer al dealer obteniendo cartas que sumen lo m치s cerca posible a 21, sin pasarse. El juego comienza con el dealer mostrando una carta y otra oculta, mientras el jugador recibe dos cartas visibles. Las cartas se sacan de un mazo infinito.\n",
    "\n",
    "**Valores de las cartas**:\n",
    "- Cartas de figuras (J, Q, K): **10 puntos**.\n",
    "- Ases: **1 o 11 puntos** (usable ace).\n",
    "- Cartas num칠ricas (2-9): Valor num칠rico.\n",
    "\n",
    "**Acciones**:\n",
    "- **0 (Stick)**: Detenerse.\n",
    "- **1 (Hit)**: Pedir una carta.\n",
    "\n",
    "**Espacio de observaci칩n**:\n",
    "- Tupla \\((suma\\_jugador, carta\\_dealer, as\\_usable)\\):\n",
    "  - **suma\\_jugador**: 4-21.\n",
    "  - **carta\\_dealer**: 1-10.\n",
    "  - **as\\_usable**: 0 o 1.\n",
    "\n",
    "**Recompensas**:\n",
    "- **+1**: Victoria.\n",
    "- **0**: Empate.\n",
    "- **-1**: Derrota.\n",
    "- **+1.5**: Blackjack natural (opcional).\n",
    "\n",
    "**Fin del episodio**:\n",
    "- Cuando el jugador se pasa de 21 o elige \"Stick\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmcX6bRC9agQ"
   },
   "source": [
    "#### **1.1.2 Generando un Baseline (0.2 puntos)**\n",
    "\n",
    "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulaci칩n 5000 veces y reporte el promedio y desviaci칩n de las recompensas. 쮺칩mo calificar칤a el performance de esta pol칤tica? 쮺칩mo podr칤a interpretar las recompensas obtenidas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9p2PrLLR9yju"
   },
   "outputs": [],
   "source": [
    "# Simulaci칩n de 5000 episodios con acciones aleatorias\n",
    "num_episodes = 5000\n",
    "rewards = []\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    observation, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Tomar una acci칩n aleatoria (0: Stick, 1: Hit)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "    \n",
    "    rewards.append(total_reward)\n",
    "\n",
    "# Calcular estad칤sticas\n",
    "mean_reward = np.mean(rewards)\n",
    "std_reward = np.std(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El promedio negativo de recompensas **-0.40** sugiere que la pol칤tica de tomar acciones aleatorias resulta en m치s derrotas que victorias. Esto implica que el jugador pierde m치s de lo que gana en promedio. La desviaci칩n est치ndar de **0.89** muestra una considerable variabilidad en los resultados, lo que indica que las recompensas fluct칰an significativamente de un episodio a otro.\n",
    "\n",
    "**Conclusi칩n**: El performance de esta pol칤tica aleatoria es pobre, ya que genera p칠rdidas m치s frecuentemente. Para mejorar, se necesitar칤a una estrategia que considere la probabilidad de ganar y optimice las acciones en consecuencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEO_dY4x_SJu"
   },
   "source": [
    "#### **1.1.3 Entrenamiento de modelo (0.2 puntos)**\n",
    "\n",
    "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `Blackjack`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m9JsFA1wGmnH"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "# Crear el modelo DQN\n",
    "model = DQN(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Entrenar el modelo durante 10000 pasos\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "model.save(\"dqn_blackjack_model\")\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "episodes = 1000\n",
    "rewards = []\n",
    "\n",
    "for _ in range(episodes):\n",
    "    observation, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action, _ = model.predict(observation, deterministic=True)\n",
    "        observation, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "    rewards.append(total_reward)\n",
    "\n",
    "mean_reward = np.mean(rewards)\n",
    "print(f\"Recompensa promedio despu칠s del entrenamiento: {mean_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-bpdb8wZID1"
   },
   "source": [
    "#### **1.1.4 Evaluaci칩n de modelo (0.2 puntos)**\n",
    "\n",
    "Repita el ejercicio 1.1.2 pero utilizando el modelo entrenado. 쮺칩mo es el performance de su agente? 쮼s mejor o peor que el escenario baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-d7d8GFf7F6"
   },
   "outputs": [],
   "source": [
    "model = DQN.load(\"dqn_blackjack_model\")\n",
    "\n",
    "# Evaluar el rendimiento del modelo entrenado\n",
    "episodes = 1000\n",
    "rewards_trained_model = []\n",
    "\n",
    "for _ in range(episodes):\n",
    "    observation, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Predecir la acci칩n con el modelo entrenado\n",
    "        action, _ = model.predict(observation, deterministic=True)\n",
    "        observation, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "    rewards_trained_model.append(total_reward)\n",
    "\n",
    "# Calcular las estad칤sticas para el modelo entrenado\n",
    "mean_reward_trained = np.mean(rewards_trained_model)\n",
    "std_reward_trained = np.std(rewards_trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El promedio negativo de recompensas **-0.06** sugiere que el modelo entrenado a칰n tiene un rendimiento ligeramente negativo, con un equilibrio entre victorias y derrotas. Esto implica que, aunque el agente ha aprendido a jugar, no genera ganancias consistentes en promedio. La desviaci칩n est치ndar de  **0.96** muestra una considerable variabilidad en los resultados, lo que indica que las recompensas fluct칰an significativamente de un episodio a otro.\n",
    "\n",
    "**Conclusi칩n**: El performance del modelo entrenado es mejor que el de la pol칤tica aleatoria, pero a칰n no es ideal. Para mejorar, se podr칤a optimizar el modelo, ajustar par치metros o explorar diferentes estrategias de aprendizaje para incrementar la tasa de victorias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RO-EsAaPAYEm"
   },
   "source": [
    "#### **1.1.5 Estudio de acciones (0.2 puntos)**\n",
    "\n",
    "Genere una funci칩n que reciba un estado y retorne la accion del agente. Luego, use esta funci칩n para entregar la acci칩n escogida frente a los siguientes escenarios:\n",
    "\n",
    "- Suma de cartas del agente es 6, dealer muestra un 7, agente no tiene tiene un as\n",
    "- Suma de cartas del agente es 19, dealer muestra un 3, agente tiene tiene un as\n",
    "\n",
    "쯉on coherentes sus acciones con las reglas del juego?\n",
    "\n",
    "Hint: 쮸 que clase de python pertenecen los estados? Pruebe a usar el m칠todo `.reset` para saberlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = env.reset()\n",
    "\n",
    "# Verificar el tipo del estado\n",
    "print(f\"Tipo de estado: {type(initial_state)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fh8XlGyzwtRp"
   },
   "outputs": [],
   "source": [
    "# Funci칩n para obtener la acci칩n del agente dada una observaci칩n\n",
    "def get_agent_action(state):\n",
    "    action, _ = model.predict(state, deterministic=True)\n",
    "    return action\n",
    "\n",
    "# Escenario 1: Suma de cartas del agente es 6, dealer muestra un 7, agente no tiene un as\n",
    "# Estado: (6, 7, 0)\n",
    "state_1 = np.array([6, 7, 0])\n",
    "\n",
    "# Escenario 2: Suma de cartas del agente es 19, dealer muestra un 3, agente tiene un as\n",
    "# Estado: (19, 3, 1)\n",
    "state_2 = np.array([19, 3, 1])\n",
    "\n",
    "# Obtener las acciones para ambos estados\n",
    "action_1 = get_agent_action(state_1)\n",
    "action_2 = get_agent_action(state_2)\n",
    "\n",
    "# Mostrar las acciones\n",
    "print(f\"Acci칩n para el escenario 1 (estado {state_1}): {'Hit' if action_1 == 1 else 'Stick'}\")\n",
    "print(f\"Acci칩n para el escenario 2 (estado {state_2}): {'Hit' if action_2 == 1 else 'Stick'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escenario 1: La suma de cartas del agente es 6, el dealer muestra un 7 y el agente no tiene un as.\n",
    "\n",
    "- Regla general: Cuando el agente tiene una suma baja, es probable que deba \"hit\" (pedir m치s cartas) hasta llegar a una suma m치s cercana a 21.\n",
    "- Esperado: El modelo deber칤a elegir hit.\n",
    "\n",
    "Escenario 2: La suma de cartas del agente es 19, el dealer muestra un 3, y el agente tiene un as.\n",
    "\n",
    "- Regla general: Con una suma alta (como 19), el agente deber칤a plantarse (stick). Tener un as como \"usable ace\" podr칤a hacer que el agente sea un poco m치s flexible, pero a칰n as칤, la estrategia general es plantarse.\n",
    "- Esperado: El modelo deber칤a elegir stick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEqCTqqroh03"
   },
   "source": [
    "### **1.2 LunarLander**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://i.redd.it/097t6tk29zf51.jpg\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Similar a la secci칩n 2.1, en esta secci칩n usted se encargar치 de implementar una gente de RL que pueda resolver el ambiente `LunarLander`.\n",
    "\n",
    "Comencemos preparando el ambiente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvQUyuZ_FtZ4"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v2\", render_mode = \"rgb_array\", continuous = True) # notar el par치metro continuous = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBU4lGX3wpN6"
   },
   "source": [
    "Noten que se especifica el par치metro `continuous = True`. 쯈ue implicancias tiene esto sobre el ambiente?\n",
    "\n",
    "Adem치s, se le facilita la funci칩n `export_gif` para el ejercicio 2.2.4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bRiWpSo9yfr9"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "def export_gif(model, n = 5):\n",
    "  '''\n",
    "  funci칩n que exporta a gif el comportamiento del agente en n episodios\n",
    "  '''\n",
    "  images = []\n",
    "  for episode in range(n):\n",
    "    obs = model.env.reset()\n",
    "    img = model.env.render()\n",
    "    done = False\n",
    "    while not done:\n",
    "      images.append(img)\n",
    "      action, _ = model.predict(obs)\n",
    "      obs, reward, done, info = model.env.step(action)\n",
    "      img = model.env.render(mode=\"rgb_array\")\n",
    "\n",
    "  imageio.mimsave(\"agent_performance.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sk5VJVppXh3N"
   },
   "source": [
    "#### **1.2.1 Descripci칩n de MDP (0.2 puntos)**\n",
    "\n",
    "Entregue una breve descripci칩n sobre el ambiente [LunarLander](https://gymnasium.farama.org/environments/box2d/lunar_lander/) y su formulaci칩n en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas. 쮺omo se distinguen las acciones de este ambiente en comparaci칩n a `Blackjack`?\n",
    "\n",
    "Nota: recuerde que se especific칩 el par치metro `continuous = True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yb-u9LUE8O9a"
   },
   "source": [
    "`escriba su respuesta ac치`\n",
    "\n",
    "### **Descripci칩n de MDP en LunarLander-v2**\n",
    "\n",
    "El entorno **LunarLander-v2** simula un problema de optimizaci칩n de la trayectoria de un cohete, donde el objetivo es aterrizar el m칩dulo en la luna. La formulaci칩n en MDP es la siguiente:\n",
    "\n",
    "#### **Estado (S)**\n",
    "El estado es un vector de 8 dimensiones que incluye:\n",
    "- **x, y**: Coordenadas del m칩dulo.\n",
    "- **vx, vy**: Velocidades en los ejes horizontal y vertical.\n",
    "- **치ngulo**: Orientaci칩n del m칩dulo.\n",
    "- **velocidad angular**: Rotaci칩n del m칩dulo.\n",
    "- **contacto con piernas**: Booleanos que indican si las piernas est치n tocando el suelo.\n",
    "\n",
    "#### **Acciones (A)**\n",
    "Existen 4 acciones posibles:\n",
    "- **0**: No hacer nada.\n",
    "- **1**: Encender el motor izquierdo (orientaci칩n).\n",
    "- **2**: Encender el motor principal (vertical).\n",
    "- **3**: Encender el motor derecho (orientaci칩n).\n",
    "\n",
    "#### **Recompensas (R)**\n",
    "La recompensa depende de la posici칩n, velocidad, y orientaci칩n del m칩dulo:\n",
    "- Posicionarse cerca de la plataforma y reducir la velocidad es premiado.\n",
    "- Se penaliza la inclinaci칩n del m칩dulo.\n",
    "- **+10 puntos por pierna** si toca el suelo.\n",
    "- Penalizaciones por el uso de motores: **-0.03** por cada cuadro con motores laterales encendidos, **-0.3** por el motor principal.\n",
    "- **+100 puntos** por aterrizaje seguro, **-100 puntos** por estrellarse.\n",
    "\n",
    "#### **Comparaci칩n con Blackjack**\n",
    "A diferencia de **Blackjack**, las acciones en **LunarLander** son f칤sicas, controlando motores para afectar la posici칩n, velocidad y orientaci칩n en un entorno 2D. En **Blackjack**, las decisiones son m치s simples y se basan en probabilidades de cartas. Adem치s, en LunarLander las acciones se manejan de forma discreta pero m치s compleja.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YChodtNQwzG2"
   },
   "source": [
    "#### **1.2.2 Generando un Baseline (0.2 puntos)**\n",
    "\n",
    "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulaci칩n 10 veces y reporte el promedio y desviaci칩n de las recompensas. 쮺칩mo calificar칤a el performance de esta pol칤tica?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bwc3A0GX7a8"
   },
   "outputs": [],
   "source": [
    "# Funci칩n para ejecutar una pol칤tica aleatoria y obtener las recompensas\n",
    "def simulate_random_policy(env, episodes=10):\n",
    "    rewards = []\n",
    "    for _ in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action = env.action_space.sample()  # Acci칩n aleatoria\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        rewards.append(total_reward)\n",
    "    return rewards\n",
    "\n",
    "# Simulaci칩n y c치lculo de estad칤sticas\n",
    "rewards = simulate_random_policy(env, episodes=10)\n",
    "mean_reward = np.mean(rewards)\n",
    "std_reward = np.std(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El promedio negativo de recompensas **-237.81** sugiere que la pol칤tica aleatoria tiene un rendimiento deficiente, con una tendencia a las p칠rdidas. Esto implica que, al no optimizar las acciones, el agente pierde m치s de lo que gana en promedio. La desviaci칩n est치ndar de **108.99** muestra una considerable variabilidad en los resultados, lo que indica que las recompensas fluct칰an significativamente de un episodio a otro.\n",
    "\n",
    "**Conclusi칩n**: El performance de esta pol칤tica aleatoria es pobre y no permite ganar consistentemente. Se necesita una estrategia m치s optimizada para mejorar el rendimiento en este entorno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQrZVQflX_5f"
   },
   "source": [
    "#### **1.2.3 Entrenamiento de modelo (0.2 puntos)**\n",
    "\n",
    "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `LunarLander` **usando 10000 timesteps de entrenamiento**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_6Ia9uoF7Hs"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "# Definir y entrenar el modelo con PPO\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "model.save(\"ppo_lunarlander_10000_steps\")\n",
    "\n",
    "# Evaluar el modelo entrenado\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "total_rewards = 0\n",
    "\n",
    "# Realizar una evaluaci칩n para ver c칩mo se desempe침a el modelo\n",
    "while not done:\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, done, _, _ = env.step(action)\n",
    "    total_rewards += reward\n",
    "\n",
    "print(f\"Recompensa total al final del episodio: {total_rewards}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3z-oIUSrlAsY"
   },
   "source": [
    "#### **1.2.4 Evaluaci칩n de modelo (0.2 puntos)**\n",
    "\n",
    "Repita el ejercicio 1.2.2 pero utilizando el modelo entrenado. 쮺칩mo es el performance de su agente? 쮼s mejor o peor que el escenario baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ophyU3KrWrwl"
   },
   "outputs": [],
   "source": [
    "# Cargar el modelo entrenado\n",
    "model = PPO.load(\"ppo_lunarlander_10000_steps\",env)\n",
    "\n",
    "# N칰mero de episodios para evaluar\n",
    "num_episodes = 10\n",
    "rewards = []\n",
    "\n",
    "# Evaluaci칩n del modelo en varios episodios\n",
    "for _ in range(num_episodes):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "    \n",
    "    rewards.append(total_reward)\n",
    "\n",
    "# Calcular el promedio y desviaci칩n est치ndar de las recompensas\n",
    "mean_reward_trained = np.mean(rewards)\n",
    "std_reward_trained = np.std(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El promedio de recompensas del modelo entrenado es **-189.59**, lo que sugiere que el modelo tiene un rendimiento moderado, aunque no es perfecto. Esto indica que el agente ha aprendido a optimizar sus acciones, pero a칰n podr칤a mejorar en algunos aspectos. La desviaci칩n est치ndar de **165.69** muestra que las recompensas fluct칰an entre episodios, lo que sugiere cierta variabilidad en el desempe침o del agente.\n",
    "\n",
    "**Comparaci칩n con la pol칤tica baseline**: Comparando este rendimiento con la pol칤tica baseline (acci칩n aleatoria), podemos concluir que el modelo entrenado tiene un rendimiento mucho mejor, ya que la pol칤tica aleatoria generaba un promedio de **-206.93** recompensas, mientras que el modelo entrenado est치 significativamente por encima de ese valor.\n",
    "\n",
    "**Conclusi칩n**: El rendimiento del agente entrenado es superior al de la pol칤tica baseline, pero a칰n existen oportunidades para mejorar. El agente ha aprendido a jugar, pero su desempe침o podr칤a optimizarse con m치s entrenamiento o ajustes en los hiperpar치metros del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6Xw4YHT3P5d"
   },
   "source": [
    "#### **1.2.5 Optimizaci칩n de modelo (0.2 puntos)**\n",
    "\n",
    "Repita los ejercicios 1.2.3 y 1.2.4 hasta obtener un nivel de recompensas promedio mayor a 50. Para esto, puede cambiar manualmente par치metros como:\n",
    "- `total_timesteps`\n",
    "- `learning_rate`\n",
    "- `batch_size`\n",
    "\n",
    "Una vez optimizado el modelo, use la funci칩n `export_gif` para estudiar el comportamiento de su agente en la resoluci칩n del ambiente y comente sobre sus resultados.\n",
    "\n",
    "Adjunte el gif generado en su entrega (mejor a칰n si adem치s adjuntan el gif en el markdown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir par치metros para la optimizaci칩n\n",
    "total_timesteps = 80000\n",
    "learning_rate = 0.0003  # Ajuste del learning rate\n",
    "batch_size = 64  # Aumento del tama침o del batch\n",
    "\n",
    "# Definir y entrenar el modelo con PPO\n",
    "model = PPO(\"MlpPolicy\", env, learning_rate=learning_rate, batch_size=batch_size, verbose=1)\n",
    "model.learn(total_timesteps=total_timesteps)\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "model.save(\"ppo_lunarlander_opt\")\n",
    "\n",
    "# Evaluar el modelo entrenado\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "total_rewards = 0\n",
    "\n",
    "# Realizar una evaluaci칩n\n",
    "while not done:\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, done, truncated, _ = env.step(action)\n",
    "    total_rewards += reward\n",
    "    \n",
    "    # Si el episodio termina o es truncado, detener\n",
    "    if done or truncated:\n",
    "        break\n",
    "\n",
    "print(f\"Recompensa total al final del episodio: {total_rewards}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aItYF6sr6F_6"
   },
   "outputs": [],
   "source": [
    "# Cargar el modelo entrenado\n",
    "model = PPO.load(\"ppo_lunarlander_opt\",env)\n",
    "\n",
    "# N칰mero de episodios para evaluar\n",
    "num_episodes = 5\n",
    "rewards = []\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Si el episodio termina o es truncado, detener\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    rewards.append(total_reward)\n",
    "\n",
    "# Calcular el promedio y desviaci칩n est치ndar de las recompensas\n",
    "mean_reward_trained = np.mean(rewards)\n",
    "std_reward_trained = np.std(rewards)\n",
    "\n",
    "print(f\"Promedio de recompensas: {mean_reward_trained:.2f}\")\n",
    "print(f\"Desviaci칩n est치ndar de recompensas: {std_reward_trained:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un GIF del comportamiento del agente optimizado\n",
    "export_gif(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El promedio de recompensas del modelo optimizado es **67.74**, lo que sugiere que el agente ha mejorado significativamente su desempe침o, alcanzando un rendimiento superior a 50 recompensas en promedio. Esto indica que el agente ha aprendido a optimizar sus acciones de manera efectiva. La desviaci칩n est치ndar de **49.35** muestra que las recompensas son m치s consistentes que antes, lo que refleja una mejora en la estabilidad del agente.\n",
    "\n",
    "**Comparaci칩n con el modelo anterior**: En comparaci칩n con el modelo entrenado previamente, el modelo optimizado ha mostrado una mejora clara en el rendimiento, ya que las recompensas promedio eran inferiores a 50 en el modelo anterior.\n",
    "\n",
    "**Conclusi칩n**: El rendimiento del modelo optimizado es mucho mejor. Los ajustes en los par치metros han permitido al agente aprender de manera m치s efectiva y mejorar su desempe침o en el entorno de LunarLander.\n",
    "\n",
    "![Agente optimizado](agent_performance.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPUY-Ktgf2BO"
   },
   "source": [
    "## **2. Large Language Models (4.0 puntos)**\n",
    "\n",
    "En esta secci칩n se enfocar치n en habilitar un Chatbot que nos permita responder preguntas 칰tiles a trav칠s de LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQ4fPRRihGLe"
   },
   "source": [
    "### **2.0 Configuraci칩n Inicial**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/uqAs9atZH58AAAAd/config-config-issue.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Como siempre, cargamos todas nuestras API KEY al entorno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ud2Xm_k-hFJn"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
    "\n",
    "if \"TAVILY_API_KEY\" not in os.environ:\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your Tavily API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rj9JvQUsgZZJ"
   },
   "source": [
    "### **2.1 Retrieval Augmented Generation (1.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://y.yarn.co/218aaa02-c47e-4ec9-b1c9-07792a06a88f_text.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "El objetivo de esta subsecci칩n es que habiliten un chatbot que pueda responder preguntas usando informaci칩n contenida en documentos PDF a trav칠s de **Retrieval Augmented Generation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrxOQroVnaZ5"
   },
   "source": [
    "#### **2.1.1 Reunir Documentos (0 puntos)**\n",
    "\n",
    "Reuna documentos PDF sobre los que hacer preguntas siguiendo las siguientes instrucciones:\n",
    "  - 2 documentos .pdf como m칤nimo.\n",
    "  - 50 p치ginas de contenido como m칤nimo entre todos los documentos.\n",
    "  - Ideas para documentos: Documentos relacionados a temas acad칠micos, laborales o de ocio. Aprovechen este ejercicio para construir algo 칰til y/o relevante para ustedes!\n",
    "  - Deben ocupar documentos reales, no pueden utilizar los mismos de la clase.\n",
    "  - Deben registrar sus documentos en la siguiente [planilla](https://docs.google.com/spreadsheets/d/1Hy1w_dOiG2UCHJ8muyxhdKPZEPrrL7BNHm6E90imIIM/edit?usp=sharing). **NO PUEDEN USAR LOS MISMOS DOCUMENTOS QUE OTRO GRUPO**\n",
    "  - **Recuerden adjuntar los documentos en su entrega**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5D1tIRCi4oJJ",
    "outputId": "39f6d4fc-63cb-4b9b-d48f-48d60df25ee5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet PyPDF2\n",
    "# Instalar bibliotecas necesarias si no est치n instaladas\n",
    "%pip install --quiet langchain\n",
    "%pip install --quiet PyPDF2 sentence-transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kzq2TjWCnu15"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Deben adjuntar un m칤nimo de 2 documentos",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mPyPDF2\u001b[39;00m\n\u001b[0;32m      3\u001b[0m doc_paths \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;66;03m# rellenar con los path a sus documentos\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(doc_paths) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeben adjuntar un m칤nimo de 2 documentos\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m total_paginas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(PyPDF2\u001b[38;5;241m.\u001b[39mPdfReader(\u001b[38;5;28mopen\u001b[39m(doc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mpages) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m doc_paths)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m total_paginas \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP치ginas insuficientes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_paginas\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Deben adjuntar un m칤nimo de 2 documentos"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "doc_paths = [] # rellenar con los path a sus documentos\n",
    "\n",
    "assert len(doc_paths) >= 2, \"Deben adjuntar un m칤nimo de 2 documentos\"\n",
    "\n",
    "total_paginas = sum(len(PyPDF2.PdfReader(open(doc, \"rb\")).pages) for doc in doc_paths)\n",
    "assert total_paginas >= 50, f\"P치ginas insuficientes: {total_paginas}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de p치ginas: 153\n"
     ]
    }
   ],
   "source": [
    "# Lista de rutas a los documentos PDF\n",
    "doc_paths = [\n",
    "    r\"C:\\Users\\joaqu\\Downloads\\c_1925.pdf\",\n",
    "    r\"C:\\Users\\joaqu\\Downloads\\constitucion_chile.pdf\"\n",
    "]\n",
    "\n",
    "# Validar n칰mero de documentos y p치ginas\n",
    "import PyPDF2\n",
    "\n",
    "assert len(doc_paths) >= 2, \"Deben adjuntar un m칤nimo de 2 documentos\"\n",
    "\n",
    "# Calcular el total de p치ginas\n",
    "total_paginas = sum(len(PyPDF2.PdfReader(open(doc, \"rb\")).pages) for doc in doc_paths)\n",
    "assert total_paginas >= 50, f\"P치ginas insuficientes: {total_paginas}\"\n",
    "print(f\"Total de p치ginas: {total_paginas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r811-P71nizA"
   },
   "source": [
    "#### **2.1.2 Vectorizar Documentos (0.2 puntos)**\n",
    "\n",
    "Vectorice los documentos y almacene sus representaciones de manera acorde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "n-yXAdCSn4JM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Documento: c_1925.pdf - Fragmentos generados: 76\n",
      "Documento: constitucion_chile.pdf - Fragmentos generados: 303\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f067d86a6c2741b48440d62441f2ef29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de los embeddings: (379, 384)\n",
      "Vectorizaci칩n completa. Total de fragmentos generados: 379\n",
      "Los vectores y metadatos se han almacenado en 'vector_store.pkl'.\n"
     ]
    }
   ],
   "source": [
    "# Instalar bibliotecas necesarias\n",
    "%pip install --quiet PyPDF2 sentence-transformers faiss-cpu\n",
    "\n",
    "# Importar librer칤as necesarias\n",
    "import PyPDF2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "\n",
    "# Funci칩n para dividir texto en fragmentos manualmente\n",
    "def dividir_texto(text, chunk_size, chunk_overlap):\n",
    "    fragments = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        fragments.append(text[start:end])\n",
    "        start += chunk_size - chunk_overlap\n",
    "    return fragments\n",
    "\n",
    "# Funci칩n para cargar y vectorizar documentos\n",
    "def cargar_y_vectorizar_documentos(doc_paths, chunk_size=1000, chunk_overlap=100):\n",
    "    # Diccionario para almacenar los textos de cada documento\n",
    "    doc_texts = {}\n",
    "\n",
    "    # Extraer texto de los documentos PDF\n",
    "    for doc in doc_paths:\n",
    "        with open(doc, \"rb\") as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text = \"\".join(page.extract_text() for page in reader.pages)\n",
    "            doc_texts[doc] = text  # Guardar el texto extra칤do en el diccionario\n",
    "\n",
    "    # Dividir textos en fragmentos manejables\n",
    "    texts = []  # Lista para almacenar los fragmentos\n",
    "    metadatas = []  # Lista para los metadatos\n",
    "\n",
    "    for doc_name, text in doc_texts.items():\n",
    "        split_text = dividir_texto(text, chunk_size, chunk_overlap)  # Fragmentar el texto\n",
    "        texts.extend(split_text)  # Agregar los fragmentos a la lista\n",
    "        metadatas.extend([{\"source\": doc_name}] * len(split_text))  # Asociar metadatos a cada fragmento\n",
    "        print(f\"Documento: {doc_name} - Fragmentos generados: {len(split_text)}\")\n",
    "\n",
    "    # Usar SentenceTransformers para generar embeddings\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    embeddings = model.encode(texts, show_progress_bar=True)\n",
    "    print(f\"Dimensiones de los embeddings: {embeddings.shape}\")\n",
    "\n",
    "    # Crear 칤ndice FAISS basado en similitud L2\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)  # A침adir los embeddings al 칤ndice\n",
    "\n",
    "    # Guardar 칤ndice FAISS y metadatos\n",
    "    with open(\"vector_store.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\"index\": index, \"texts\": texts, \"metadatas\": metadatas}, f)\n",
    "\n",
    "    return index, len(texts)\n",
    "\n",
    "# Rutas de documentos (aseg칰rate de haber subido estos archivos)\n",
    "doc_paths = [\"c_1925.pdf\", \"constitucion_chile.pdf\"]  # Cambia seg칰n tus archivos\n",
    "\n",
    "# Ejecutar el proceso de vectorizaci칩n\n",
    "index, total_chunks = cargar_y_vectorizar_documentos(doc_paths)\n",
    "\n",
    "print(f\"Vectorizaci칩n completa. Total de fragmentos generados: {total_chunks}\")\n",
    "print(\"Los vectores y metadatos se han almacenado en 'vector_store.pkl'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAUkP5zrnyBK"
   },
   "source": [
    "#### **2.1.3 Habilitar RAG (0.3 puntos)**\n",
    "\n",
    "Habilite la soluci칩n RAG a trav칠s de una *chain* y gu치rdela en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPIySdDFn99l"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycg5S5i_n-kL"
   },
   "source": [
    "#### **2.1.4 Verificaci칩n de respuestas (0.5 puntos)**\n",
    "\n",
    "Genere un listado de 3 tuplas (\"pregunta\", \"respuesta correcta\") y analice la respuesta de su soluci칩n para cada una. 쯉u soluci칩n RAG entrega las respuestas que esperaba?\n",
    "\n",
    "Ejemplo de tupla:\n",
    "- Pregunta: 쯈ui칠n es el presidente de Chile?\n",
    "- Respuesta correcta: El presidente de Chile es Gabriel Boric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_UiEn1hoZYR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8d5zTMHoUgF"
   },
   "source": [
    "#### **2.1.5 Sensibilidad de Hiperpar치metros (0.5 puntos)**\n",
    "\n",
    "Extienda el an치lisis del punto 2.1.4 analizando c칩mo cambian las respuestas entregadas cambiando los siguientes hiperpar치metros:\n",
    "- `Tama침o del chunk`. (*쮺칩mo repercute que los chunks sean mas grandes o chicos?*)\n",
    "- `La cantidad de chunks recuperados`. (*쯈u칠 pasa si se devuelven muchos/pocos chunks?*)\n",
    "- `El tipo de b칰squeda`. (*쮺칩mo afecta el tipo de b칰squeda a las respuestas de mi RAG?*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UDh_QgeXLGHc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENJiPPM0giX8"
   },
   "source": [
    "### **2.2 Agentes (1.0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/rcqnN2aJCSEAAAAd/secret-agent-man.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Similar a la secci칩n anterior, en esta secci칩n se busca habilitar **Agentes** para obtener informaci칩n a trav칠s de tools y as칤 responder la pregunta del usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V47l7Mjfrk0N"
   },
   "source": [
    "#### **2.2.1 Tool de Tavily (0.2 puntos)**\n",
    "\n",
    "Generar una *tool* que pueda hacer consultas al motor de b칰squeda **Tavily**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6SLKwcWr0AG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SonB1A-9rtRq"
   },
   "source": [
    "#### **2.2.2 Tool de Wikipedia (0.2 puntos)**\n",
    "\n",
    "Generar una *tool* que pueda hacer consultas a **Wikipedia**.\n",
    "\n",
    "*Hint: Le puede ser de ayuda el siguiente [link](https://python.langchain.com/v0.1/docs/modules/tools/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehJJpoqsr26-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvUIMdX6r0ne"
   },
   "source": [
    "#### **2.2.3 Crear Agente (0.3 puntos)**\n",
    "\n",
    "Crear un agente que pueda responder preguntas preguntas usando las *tools* antes generadas. Aseg칰rese que su agente responda en espa침ol. Por 칰ltimo, guarde el agente en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pD1_n0wrsDI5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKV0JxK3r-XG"
   },
   "source": [
    "#### **2.2.4 Verificaci칩n de respuestas (0.3 puntos)**\n",
    "\n",
    "Pruebe el funcionamiento de su agente y aseg칰rese que el agente est칠 ocupando correctamente las tools disponibles. 쮼n qu칠 casos el agente deber칤a ocupar la tool de Tavily? 쮼n qu칠 casos deber칤a ocupar la tool de Wikipedia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pqo2dsxvywW_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZbDTYiogquv"
   },
   "source": [
    "### **2.3 Multi Agente (1.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/r7QMJLxU4BoAAAAd/this-is-getting-out-of-hand-star-wars.gif\"\n",
    "\" width=\"450\">\n",
    "</p>\n",
    "\n",
    "El objetivo de esta subsecci칩n es encapsular las funcionalidades creadas en una soluci칩n multiagente con un **supervisor**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-iUfH0WvI6m"
   },
   "source": [
    "#### **2.3.1 Generando Tools (0.5 puntos)**\n",
    "\n",
    "Transforme la soluci칩n RAG de la secci칩n 2.1 y el agente de la secci칩n 2.2 a *tools* (una tool por cada uno)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pw1cfTtvv1AZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQYNjT_0vPCg"
   },
   "source": [
    "#### **2.3.2 Agente Supervisor (0.5 puntos)**\n",
    "\n",
    "Habilite un agente que tenga acceso a las tools del punto anterior y pueda responder preguntas relacionadas. Almacene este agente en una variable llamada supervisor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yv2ZY0BAv1RD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea3zWlvyvY7K"
   },
   "source": [
    "#### **2.3.3 Verificaci칩n de respuestas (0.25 puntos)**\n",
    "\n",
    "Pruebe el funcionamiento de su agente repitiendo las preguntas realizadas en las secciones 2.1.4 y 2.2.4 y comente sus resultados. 쮺칩mo var칤an las respuestas bajo este enfoque?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_1t0zkgv1qW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qb8bdAmYvgwn"
   },
   "source": [
    "#### **2.3.4 An치lisis (0.25 puntos)**\n",
    "\n",
    "쯈u칠 diferencias tiene este enfoque con la soluci칩n *Router* vista en clases? Nombre al menos una ventaja y desventaja."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAUlJxqoLK5r"
   },
   "source": [
    "`escriba su respuesta ac치`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JWVSuWiZ8Mj"
   },
   "source": [
    "### **2.4 Memoria (Bonus +0.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/Gs95aiElrscAAAAd/memory-unlocked-ratatouille-critic.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Una de las principales falencias de las soluciones que hemos visto hasta ahora es que nuestro chat no responde las interacciones anteriores, por ejemplo:\n",
    "\n",
    "- Pregunta 1: \"Hola! mi nombre es Sebasti치n\"\n",
    "  - Respuesta esperada: \"Hola Sebasti치n! ...\"\n",
    "- Pregunta 2: \"Cual es mi nombre?\"\n",
    "  - Respuesta actual: \"Lo siento pero no conozco tu nombre :(\"\n",
    "  - **Respuesta esperada: \"Tu nombre es Sebasti치n\"**\n",
    "\n",
    "Para solucionar esto, se les solicita agregar un componente de **memoria** a la soluci칩n entregada en el punto 2.3.\n",
    "\n",
    "**Nota: El Bonus es v치lido <u>s칩lo para la secci칩n 2 de Large Language Models.</u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6Y7tIPJLPfB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFc3jBT5g0kT"
   },
   "source": [
    "### **2.5 Despliegue (0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/IytHqOp52EsAAAAd/you-get-a-deploy-deploy.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Una vez tengan los puntos anteriores finalizados, toca la etapa de dar a conocer lo que hicimos! Para eso, vamos a desplegar nuestro modelo a trav칠s de `gradio`, una librer칤a especializada en el levantamiento r치pido de demos basadas en ML.\n",
    "\n",
    "Primero instalamos la librer칤a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8TsvnCPbkIA"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJBztEUovKsF"
   },
   "source": [
    "Luego s칩lo deben ejecutar el siguiente c칩digo e interactuar con la interfaz a trav칠s del notebook o del link generado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3KedQSvg1-n"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "\n",
    "def agent_response(message, history):\n",
    "  '''\n",
    "  Funci칩n para gradio, recibe mensaje e historial, devuelte la respuesta del chatbot.\n",
    "  '''\n",
    "  # get chatbot response\n",
    "  response = ... # rellenar con la respuesta de su chat\n",
    "\n",
    "  # assert\n",
    "  assert type(response) == str, \"output de route_question debe ser string\"\n",
    "\n",
    "  # \"streaming\" response\n",
    "  for i in range(len(response)):\n",
    "    time.sleep(0.015)\n",
    "    yield response[: i+1]\n",
    "\n",
    "gr.ChatInterface(\n",
    "    agent_response,\n",
    "    type=\"messages\",\n",
    "    title=\"Chatbot MDS7202\", # Pueden cambiar esto si lo desean\n",
    "    description=\"Hola! Soy un chatbot muy 칰til :)\", # tambi칠n la descripci칩n\n",
    "    theme=\"soft\",\n",
    "    ).launch(\n",
    "        share=True, # pueden compartir el link a sus amig@s para que interactuen con su chat!\n",
    "        debug = False,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
