{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyPTffTLug7i"
   },
   "source": [
    "# **Laboratorio 11: LLM y Agentes Autónomos 🤖**\n",
    "\n",
    "MDS7202: Laboratorio de Programación Científica para Ciencia de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pbWVyntzbvL"
   },
   "source": [
    "### **Cuerpo Docente:**\n",
    "\n",
    "- Profesores: Ignacio Meza, Sebastián Tinoco\n",
    "- Auxiliar: Eduardo Moya\n",
    "- Ayudantes: Nicolás Ojeda, Melanie Peña, Valentina Rojas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dy6ikgVYzghB"
   },
   "source": [
    "### Equipo: **SUPER IMPORTANTE - notebooks sin nombre no serán revisados**\n",
    "\n",
    "- Nombre de alumno 1: Joaquín De Groote\n",
    "- Nombre de alumno 2: Vicente Pinochet R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMJ-owchzjFf"
   },
   "source": [
    "### **Link de repositorio de GitHub:** [Insertar Enlace](https://github.com/Qajirr/MDS7202-Labs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUuwsXrKzmkK"
   },
   "source": [
    "## **Temas a tratar**\n",
    "\n",
    "- Reinforcement Learning\n",
    "- Large Language Models\n",
    "\n",
    "## **Reglas:**\n",
    "\n",
    "- **Grupos de 2 personas**\n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente serán respondidos por este medio.\n",
    "- Prohibidas las copias.\n",
    "- Pueden usar cualquer matrial del curso que estimen conveniente.\n",
    "\n",
    "### **Objetivos principales del laboratorio**\n",
    "\n",
    "- Resolución de problemas secuenciales usando Reinforcement Learning\n",
    "- Habilitar un Chatbot para entregar respuestas útiles usando Large Language Models.\n",
    "\n",
    "El laboratorio deberá ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al máximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante más eficientes que los iteradores nativos sobre DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hmHHQ9BuyAG"
   },
   "source": [
    "## **1. Reinforcement Learning (2.0 puntos)**\n",
    "\n",
    "En esta sección van a usar métodos de RL para resolver dos problemas interesantes: `Blackjack` y `LunarLander`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gOcejYb6uzOO"
   },
   "outputs": [],
   "source": [
    "!pip install -qqq gymnasium stable_baselines3\n",
    "!pip install -qqq swig\n",
    "!pip install -qqq gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBPet_Mq8dX9"
   },
   "source": [
    "### **1.1 Blackjack (1.0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.recreoviral.com/wp-content/uploads/2016/08/s3.amazonaws.com-Math.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "La idea de esta subsección es que puedan implementar métodos de RL y así generar una estrategia para jugar el clásico juego Blackjack y de paso puedan ~~hacerse millonarios~~ aprender a resolver problemas mediante RL.\n",
    "\n",
    "Comencemos primero preparando el ambiente. El siguiente bloque de código transforma las observaciones del ambiente a `np.array`:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</p>\n",
    "\n",
    "La idea de esta subsección es que puedan implementar métodos de RL y así generar una estrategia para jugar el clásico juego Blackjack y de paso puedan ~~hacerse millonarios~~ aprender a resolver problemas mediante RL.\n",
    "\n",
    "Comencemos primero preparando el ambiente. El siguiente bloque de código transforma las observaciones del ambiente a `np.array`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LpZ8bBKk9ZlU"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import MultiDiscrete\n",
    "import numpy as np\n",
    "\n",
    "class FlattenObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(FlattenObservation, self).__init__(env)\n",
    "        self.observation_space = MultiDiscrete(np.array([32, 11, 2]))\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.array(observation).flatten()\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = gym.make(\"Blackjack-v1\")\n",
    "env = FlattenObservation(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJ6J1_-Y9nHO"
   },
   "source": [
    "#### **1.1.1 Descripción de MDP (0.2 puntos)**\n",
    "\n",
    "Entregue una breve descripción sobre el ambiente [Blackjack](https://gymnasium.farama.org/environments/toy_text/blackjack/) y su formulación en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5i1Wt1p770x"
   },
   "source": [
    "`escriba su respuesta acá`\n",
    "\n",
    "**Blackjack - Descripción del Ambiente**\n",
    "\n",
    "El objetivo en **Blackjack** es vencer al dealer obteniendo cartas que sumen lo más cerca posible a 21, sin pasarse. El juego comienza con el dealer mostrando una carta y otra oculta, mientras el jugador recibe dos cartas visibles. Las cartas se sacan de un mazo infinito.\n",
    "\n",
    "**Valores de las cartas**:\n",
    "- Cartas de figuras (J, Q, K): **10 puntos**.\n",
    "- Ases: **1 o 11 puntos** (usable ace).\n",
    "- Cartas numéricas (2-9): Valor numérico.\n",
    "\n",
    "**Acciones**:\n",
    "- **0 (Stick)**: Detenerse.\n",
    "- **1 (Hit)**: Pedir una carta.\n",
    "\n",
    "**Espacio de observación**:\n",
    "- Tupla \\((suma\\_jugador, carta\\_dealer, as\\_usable)\\):\n",
    "  - **suma\\_jugador**: 4-21.\n",
    "  - **carta\\_dealer**: 1-10.\n",
    "  - **as\\_usable**: 0 o 1.\n",
    "\n",
    "**Recompensas**:\n",
    "- **+1**: Victoria.\n",
    "- **0**: Empate.\n",
    "- **-1**: Derrota.\n",
    "- **+1.5**: Blackjack natural (opcional).\n",
    "\n",
    "**Fin del episodio**:\n",
    "- Cuando el jugador se pasa de 21 o elige \"Stick\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmcX6bRC9agQ"
   },
   "source": [
    "#### **1.1.2 Generando un Baseline (0.2 puntos)**\n",
    "\n",
    "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulación 5000 veces y reporte el promedio y desviación de las recompensas. ¿Cómo calificaría el performance de esta política? ¿Cómo podría interpretar las recompensas obtenidas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9p2PrLLR9yju"
   },
   "outputs": [],
   "source": [
    "# Simulación de 5000 episodios con acciones aleatorias\n",
    "num_episodes = 5000\n",
    "rewards = []\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    observation, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Tomar una acción aleatoria (0: Stick, 1: Hit)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "    \n",
    "    rewards.append(total_reward)\n",
    "\n",
    "# Calcular estadísticas\n",
    "mean_reward = np.mean(rewards)\n",
    "std_reward = np.std(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El promedio negativo de recompensas **-0.40** sugiere que la política de tomar acciones aleatorias resulta en más derrotas que victorias. Esto implica que el jugador pierde más de lo que gana en promedio. La desviación estándar de **0.89** muestra una considerable variabilidad en los resultados, lo que indica que las recompensas fluctúan significativamente de un episodio a otro.\n",
    "\n",
    "**Conclusión**: El performance de esta política aleatoria es pobre, ya que genera pérdidas más frecuentemente. Para mejorar, se necesitaría una estrategia que considere la probabilidad de ganar y optimice las acciones en consecuencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEO_dY4x_SJu"
   },
   "source": [
    "#### **1.1.3 Entrenamiento de modelo (0.2 puntos)**\n",
    "\n",
    "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `Blackjack`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m9JsFA1wGmnH"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "# Crear el modelo DQN\n",
    "model = DQN(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Entrenar el modelo durante 10000 pasos\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "model.save(\"dqn_blackjack_model\")\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "episodes = 1000\n",
    "rewards = []\n",
    "\n",
    "for _ in range(episodes):\n",
    "    observation, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action, _ = model.predict(observation, deterministic=True)\n",
    "        observation, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "    rewards.append(total_reward)\n",
    "\n",
    "mean_reward = np.mean(rewards)\n",
    "print(f\"Recompensa promedio después del entrenamiento: {mean_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-bpdb8wZID1"
   },
   "source": [
    "#### **1.1.4 Evaluación de modelo (0.2 puntos)**\n",
    "\n",
    "Repita el ejercicio 1.1.2 pero utilizando el modelo entrenado. ¿Cómo es el performance de su agente? ¿Es mejor o peor que el escenario baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-d7d8GFf7F6"
   },
   "outputs": [],
   "source": [
    "model = DQN.load(\"dqn_blackjack_model\")\n",
    "\n",
    "# Evaluar el rendimiento del modelo entrenado\n",
    "episodes = 1000\n",
    "rewards_trained_model = []\n",
    "\n",
    "for _ in range(episodes):\n",
    "    observation, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Predecir la acción con el modelo entrenado\n",
    "        action, _ = model.predict(observation, deterministic=True)\n",
    "        observation, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "    rewards_trained_model.append(total_reward)\n",
    "\n",
    "# Calcular las estadísticas para el modelo entrenado\n",
    "mean_reward_trained = np.mean(rewards_trained_model)\n",
    "std_reward_trained = np.std(rewards_trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El promedio negativo de recompensas **-0.06** sugiere que el modelo entrenado aún tiene un rendimiento ligeramente negativo, con un equilibrio entre victorias y derrotas. Esto implica que, aunque el agente ha aprendido a jugar, no genera ganancias consistentes en promedio. La desviación estándar de  **0.96** muestra una considerable variabilidad en los resultados, lo que indica que las recompensas fluctúan significativamente de un episodio a otro.\n",
    "\n",
    "**Conclusión**: El performance del modelo entrenado es mejor que el de la política aleatoria, pero aún no es ideal. Para mejorar, se podría optimizar el modelo, ajustar parámetros o explorar diferentes estrategias de aprendizaje para incrementar la tasa de victorias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RO-EsAaPAYEm"
   },
   "source": [
    "#### **1.1.5 Estudio de acciones (0.2 puntos)**\n",
    "\n",
    "Genere una función que reciba un estado y retorne la accion del agente. Luego, use esta función para entregar la acción escogida frente a los siguientes escenarios:\n",
    "\n",
    "- Suma de cartas del agente es 6, dealer muestra un 7, agente no tiene tiene un as\n",
    "- Suma de cartas del agente es 19, dealer muestra un 3, agente tiene tiene un as\n",
    "\n",
    "¿Son coherentes sus acciones con las reglas del juego?\n",
    "\n",
    "Hint: ¿A que clase de python pertenecen los estados? Pruebe a usar el método `.reset` para saberlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = env.reset()\n",
    "\n",
    "# Verificar el tipo del estado\n",
    "print(f\"Tipo de estado: {type(initial_state)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fh8XlGyzwtRp"
   },
   "outputs": [],
   "source": [
    "# Función para obtener la acción del agente dada una observación\n",
    "def get_agent_action(state):\n",
    "    action, _ = model.predict(state, deterministic=True)\n",
    "    return action\n",
    "\n",
    "# Escenario 1: Suma de cartas del agente es 6, dealer muestra un 7, agente no tiene un as\n",
    "# Estado: (6, 7, 0)\n",
    "state_1 = np.array([6, 7, 0])\n",
    "\n",
    "# Escenario 2: Suma de cartas del agente es 19, dealer muestra un 3, agente tiene un as\n",
    "# Estado: (19, 3, 1)\n",
    "state_2 = np.array([19, 3, 1])\n",
    "\n",
    "# Obtener las acciones para ambos estados\n",
    "action_1 = get_agent_action(state_1)\n",
    "action_2 = get_agent_action(state_2)\n",
    "\n",
    "# Mostrar las acciones\n",
    "print(f\"Acción para el escenario 1 (estado {state_1}): {'Hit' if action_1 == 1 else 'Stick'}\")\n",
    "print(f\"Acción para el escenario 2 (estado {state_2}): {'Hit' if action_2 == 1 else 'Stick'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escenario 1: La suma de cartas del agente es 6, el dealer muestra un 7 y el agente no tiene un as.\n",
    "\n",
    "- Regla general: Cuando el agente tiene una suma baja, es probable que deba \"hit\" (pedir más cartas) hasta llegar a una suma más cercana a 21.\n",
    "- Esperado: El modelo debería elegir hit.\n",
    "\n",
    "Escenario 2: La suma de cartas del agente es 19, el dealer muestra un 3, y el agente tiene un as.\n",
    "\n",
    "- Regla general: Con una suma alta (como 19), el agente debería plantarse (stick). Tener un as como \"usable ace\" podría hacer que el agente sea un poco más flexible, pero aún así, la estrategia general es plantarse.\n",
    "- Esperado: El modelo debería elegir stick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEqCTqqroh03"
   },
   "source": [
    "### **1.2 LunarLander**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://i.redd.it/097t6tk29zf51.jpg\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Similar a la sección 2.1, en esta sección usted se encargará de implementar una gente de RL que pueda resolver el ambiente `LunarLander`.\n",
    "\n",
    "Comencemos preparando el ambiente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvQUyuZ_FtZ4"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v2\", render_mode = \"rgb_array\", continuous = True) # notar el parámetro continuous = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBU4lGX3wpN6"
   },
   "source": [
    "Noten que se especifica el parámetro `continuous = True`. ¿Que implicancias tiene esto sobre el ambiente?\n",
    "\n",
    "Además, se le facilita la función `export_gif` para el ejercicio 2.2.4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bRiWpSo9yfr9"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "def export_gif(model, n = 5):\n",
    "  '''\n",
    "  función que exporta a gif el comportamiento del agente en n episodios\n",
    "  '''\n",
    "  images = []\n",
    "  for episode in range(n):\n",
    "    obs = model.env.reset()\n",
    "    img = model.env.render()\n",
    "    done = False\n",
    "    while not done:\n",
    "      images.append(img)\n",
    "      action, _ = model.predict(obs)\n",
    "      obs, reward, done, info = model.env.step(action)\n",
    "      img = model.env.render(mode=\"rgb_array\")\n",
    "\n",
    "  imageio.mimsave(\"agent_performance.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sk5VJVppXh3N"
   },
   "source": [
    "#### **1.2.1 Descripción de MDP (0.2 puntos)**\n",
    "\n",
    "Entregue una breve descripción sobre el ambiente [LunarLander](https://gymnasium.farama.org/environments/box2d/lunar_lander/) y su formulación en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas. ¿Como se distinguen las acciones de este ambiente en comparación a `Blackjack`?\n",
    "\n",
    "Nota: recuerde que se especificó el parámetro `continuous = True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yb-u9LUE8O9a"
   },
   "source": [
    "`escriba su respuesta acá`\n",
    "\n",
    "### **Descripción de MDP en LunarLander-v2**\n",
    "\n",
    "El entorno **LunarLander-v2** simula un problema de optimización de la trayectoria de un cohete, donde el objetivo es aterrizar el módulo en la luna. La formulación en MDP es la siguiente:\n",
    "\n",
    "#### **Estado (S)**\n",
    "El estado es un vector de 8 dimensiones que incluye:\n",
    "- **x, y**: Coordenadas del módulo.\n",
    "- **vx, vy**: Velocidades en los ejes horizontal y vertical.\n",
    "- **ángulo**: Orientación del módulo.\n",
    "- **velocidad angular**: Rotación del módulo.\n",
    "- **contacto con piernas**: Booleanos que indican si las piernas están tocando el suelo.\n",
    "\n",
    "#### **Acciones (A)**\n",
    "Existen 4 acciones posibles:\n",
    "- **0**: No hacer nada.\n",
    "- **1**: Encender el motor izquierdo (orientación).\n",
    "- **2**: Encender el motor principal (vertical).\n",
    "- **3**: Encender el motor derecho (orientación).\n",
    "\n",
    "#### **Recompensas (R)**\n",
    "La recompensa depende de la posición, velocidad, y orientación del módulo:\n",
    "- Posicionarse cerca de la plataforma y reducir la velocidad es premiado.\n",
    "- Se penaliza la inclinación del módulo.\n",
    "- **+10 puntos por pierna** si toca el suelo.\n",
    "- Penalizaciones por el uso de motores: **-0.03** por cada cuadro con motores laterales encendidos, **-0.3** por el motor principal.\n",
    "- **+100 puntos** por aterrizaje seguro, **-100 puntos** por estrellarse.\n",
    "\n",
    "#### **Comparación con Blackjack**\n",
    "A diferencia de **Blackjack**, las acciones en **LunarLander** son físicas, controlando motores para afectar la posición, velocidad y orientación en un entorno 2D. En **Blackjack**, las decisiones son más simples y se basan en probabilidades de cartas. Además, en LunarLander las acciones se manejan de forma discreta pero más compleja.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YChodtNQwzG2"
   },
   "source": [
    "#### **1.2.2 Generando un Baseline (0.2 puntos)**\n",
    "\n",
    "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulación 10 veces y reporte el promedio y desviación de las recompensas. ¿Cómo calificaría el performance de esta política?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bwc3A0GX7a8"
   },
   "outputs": [],
   "source": [
    "# Función para ejecutar una política aleatoria y obtener las recompensas\n",
    "def simulate_random_policy(env, episodes=10):\n",
    "    rewards = []\n",
    "    for _ in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action = env.action_space.sample()  # Acción aleatoria\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        rewards.append(total_reward)\n",
    "    return rewards\n",
    "\n",
    "# Simulación y cálculo de estadísticas\n",
    "rewards = simulate_random_policy(env, episodes=10)\n",
    "mean_reward = np.mean(rewards)\n",
    "std_reward = np.std(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El promedio negativo de recompensas **-237.81** sugiere que la política aleatoria tiene un rendimiento deficiente, con una tendencia a las pérdidas. Esto implica que, al no optimizar las acciones, el agente pierde más de lo que gana en promedio. La desviación estándar de **108.99** muestra una considerable variabilidad en los resultados, lo que indica que las recompensas fluctúan significativamente de un episodio a otro.\n",
    "\n",
    "**Conclusión**: El performance de esta política aleatoria es pobre y no permite ganar consistentemente. Se necesita una estrategia más optimizada para mejorar el rendimiento en este entorno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQrZVQflX_5f"
   },
   "source": [
    "#### **1.2.3 Entrenamiento de modelo (0.2 puntos)**\n",
    "\n",
    "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `LunarLander` **usando 10000 timesteps de entrenamiento**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_6Ia9uoF7Hs"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "# Definir y entrenar el modelo con PPO\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "model.save(\"ppo_lunarlander_10000_steps\")\n",
    "\n",
    "# Evaluar el modelo entrenado\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "total_rewards = 0\n",
    "\n",
    "# Realizar una evaluación para ver cómo se desempeña el modelo\n",
    "while not done:\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, done, _, _ = env.step(action)\n",
    "    total_rewards += reward\n",
    "\n",
    "print(f\"Recompensa total al final del episodio: {total_rewards}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3z-oIUSrlAsY"
   },
   "source": [
    "#### **1.2.4 Evaluación de modelo (0.2 puntos)**\n",
    "\n",
    "Repita el ejercicio 1.2.2 pero utilizando el modelo entrenado. ¿Cómo es el performance de su agente? ¿Es mejor o peor que el escenario baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ophyU3KrWrwl"
   },
   "outputs": [],
   "source": [
    "# Cargar el modelo entrenado\n",
    "model = PPO.load(\"ppo_lunarlander_10000_steps\",env)\n",
    "\n",
    "# Número de episodios para evaluar\n",
    "num_episodes = 10\n",
    "rewards = []\n",
    "\n",
    "# Evaluación del modelo en varios episodios\n",
    "for _ in range(num_episodes):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "    \n",
    "    rewards.append(total_reward)\n",
    "\n",
    "# Calcular el promedio y desviación estándar de las recompensas\n",
    "mean_reward_trained = np.mean(rewards)\n",
    "std_reward_trained = np.std(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El promedio de recompensas del modelo entrenado es **-189.59**, lo que sugiere que el modelo tiene un rendimiento moderado, aunque no es perfecto. Esto indica que el agente ha aprendido a optimizar sus acciones, pero aún podría mejorar en algunos aspectos. La desviación estándar de **165.69** muestra que las recompensas fluctúan entre episodios, lo que sugiere cierta variabilidad en el desempeño del agente.\n",
    "\n",
    "**Comparación con la política baseline**: Comparando este rendimiento con la política baseline (acción aleatoria), podemos concluir que el modelo entrenado tiene un rendimiento mucho mejor, ya que la política aleatoria generaba un promedio de **-206.93** recompensas, mientras que el modelo entrenado está significativamente por encima de ese valor.\n",
    "\n",
    "**Conclusión**: El rendimiento del agente entrenado es superior al de la política baseline, pero aún existen oportunidades para mejorar. El agente ha aprendido a jugar, pero su desempeño podría optimizarse con más entrenamiento o ajustes en los hiperparámetros del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6Xw4YHT3P5d"
   },
   "source": [
    "#### **1.2.5 Optimización de modelo (0.2 puntos)**\n",
    "\n",
    "Repita los ejercicios 1.2.3 y 1.2.4 hasta obtener un nivel de recompensas promedio mayor a 50. Para esto, puede cambiar manualmente parámetros como:\n",
    "- `total_timesteps`\n",
    "- `learning_rate`\n",
    "- `batch_size`\n",
    "\n",
    "Una vez optimizado el modelo, use la función `export_gif` para estudiar el comportamiento de su agente en la resolución del ambiente y comente sobre sus resultados.\n",
    "\n",
    "Adjunte el gif generado en su entrega (mejor aún si además adjuntan el gif en el markdown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir parámetros para la optimización\n",
    "total_timesteps = 80000\n",
    "learning_rate = 0.0003  # Ajuste del learning rate\n",
    "batch_size = 64  # Aumento del tamaño del batch\n",
    "\n",
    "# Definir y entrenar el modelo con PPO\n",
    "model = PPO(\"MlpPolicy\", env, learning_rate=learning_rate, batch_size=batch_size, verbose=1)\n",
    "model.learn(total_timesteps=total_timesteps)\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "model.save(\"ppo_lunarlander_opt\")\n",
    "\n",
    "# Evaluar el modelo entrenado\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "total_rewards = 0\n",
    "\n",
    "# Realizar una evaluación\n",
    "while not done:\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, done, truncated, _ = env.step(action)\n",
    "    total_rewards += reward\n",
    "    \n",
    "    # Si el episodio termina o es truncado, detener\n",
    "    if done or truncated:\n",
    "        break\n",
    "\n",
    "print(f\"Recompensa total al final del episodio: {total_rewards}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aItYF6sr6F_6"
   },
   "outputs": [],
   "source": [
    "# Cargar el modelo entrenado\n",
    "model = PPO.load(\"ppo_lunarlander_opt\",env)\n",
    "\n",
    "# Número de episodios para evaluar\n",
    "num_episodes = 5\n",
    "rewards = []\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Si el episodio termina o es truncado, detener\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    rewards.append(total_reward)\n",
    "\n",
    "# Calcular el promedio y desviación estándar de las recompensas\n",
    "mean_reward_trained = np.mean(rewards)\n",
    "std_reward_trained = np.std(rewards)\n",
    "\n",
    "print(f\"Promedio de recompensas: {mean_reward_trained:.2f}\")\n",
    "print(f\"Desviación estándar de recompensas: {std_reward_trained:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un GIF del comportamiento del agente optimizado\n",
    "export_gif(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El promedio de recompensas del modelo optimizado es **67.74**, lo que sugiere que el agente ha mejorado significativamente su desempeño, alcanzando un rendimiento superior a 50 recompensas en promedio. Esto indica que el agente ha aprendido a optimizar sus acciones de manera efectiva. La desviación estándar de **49.35** muestra que las recompensas son más consistentes que antes, lo que refleja una mejora en la estabilidad del agente.\n",
    "\n",
    "**Comparación con el modelo anterior**: En comparación con el modelo entrenado previamente, el modelo optimizado ha mostrado una mejora clara en el rendimiento, ya que las recompensas promedio eran inferiores a 50 en el modelo anterior.\n",
    "\n",
    "**Conclusión**: El rendimiento del modelo optimizado es mucho mejor. Los ajustes en los parámetros han permitido al agente aprender de manera más efectiva y mejorar su desempeño en el entorno de LunarLander.\n",
    "\n",
    "![Agente optimizado](agent_performance.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPUY-Ktgf2BO"
   },
   "source": [
    "## **2. Large Language Models (4.0 puntos)**\n",
    "\n",
    "En esta sección se enfocarán en habilitar un Chatbot que nos permita responder preguntas útiles a través de LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQ4fPRRihGLe"
   },
   "source": [
    "### **2.0 Configuración Inicial**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/uqAs9atZH58AAAAd/config-config-issue.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Como siempre, cargamos todas nuestras API KEY al entorno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ud2Xm_k-hFJn"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
    "\n",
    "if \"TAVILY_API_KEY\" not in os.environ:\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your Tavily API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rj9JvQUsgZZJ"
   },
   "source": [
    "### **2.1 Retrieval Augmented Generation (1.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://y.yarn.co/218aaa02-c47e-4ec9-b1c9-07792a06a88f_text.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "El objetivo de esta subsección es que habiliten un chatbot que pueda responder preguntas usando información contenida en documentos PDF a través de **Retrieval Augmented Generation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrxOQroVnaZ5"
   },
   "source": [
    "#### **2.1.1 Reunir Documentos (0 puntos)**\n",
    "\n",
    "Reuna documentos PDF sobre los que hacer preguntas siguiendo las siguientes instrucciones:\n",
    "  - 2 documentos .pdf como mínimo.\n",
    "  - 50 páginas de contenido como mínimo entre todos los documentos.\n",
    "  - Ideas para documentos: Documentos relacionados a temas académicos, laborales o de ocio. Aprovechen este ejercicio para construir algo útil y/o relevante para ustedes!\n",
    "  - Deben ocupar documentos reales, no pueden utilizar los mismos de la clase.\n",
    "  - Deben registrar sus documentos en la siguiente [planilla](https://docs.google.com/spreadsheets/d/1Hy1w_dOiG2UCHJ8muyxhdKPZEPrrL7BNHm6E90imIIM/edit?usp=sharing). **NO PUEDEN USAR LOS MISMOS DOCUMENTOS QUE OTRO GRUPO**\n",
    "  - **Recuerden adjuntar los documentos en su entrega**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5D1tIRCi4oJJ",
    "outputId": "39f6d4fc-63cb-4b9b-d48f-48d60df25ee5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet PyPDF2\n",
    "# Instalar bibliotecas necesarias si no están instaladas\n",
    "%pip install --quiet langchain\n",
    "%pip install --quiet PyPDF2 sentence-transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kzq2TjWCnu15"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Deben adjuntar un mínimo de 2 documentos",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mPyPDF2\u001b[39;00m\n\u001b[0;32m      3\u001b[0m doc_paths \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;66;03m# rellenar con los path a sus documentos\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(doc_paths) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeben adjuntar un mínimo de 2 documentos\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m total_paginas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(PyPDF2\u001b[38;5;241m.\u001b[39mPdfReader(\u001b[38;5;28mopen\u001b[39m(doc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mpages) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m doc_paths)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m total_paginas \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPáginas insuficientes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_paginas\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Deben adjuntar un mínimo de 2 documentos"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "doc_paths = [] # rellenar con los path a sus documentos\n",
    "\n",
    "assert len(doc_paths) >= 2, \"Deben adjuntar un mínimo de 2 documentos\"\n",
    "\n",
    "total_paginas = sum(len(PyPDF2.PdfReader(open(doc, \"rb\")).pages) for doc in doc_paths)\n",
    "assert total_paginas >= 50, f\"Páginas insuficientes: {total_paginas}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de páginas: 153\n"
     ]
    }
   ],
   "source": [
    "# Lista de rutas a los documentos PDF\n",
    "doc_paths = [\n",
    "    r\"C:\\Users\\joaqu\\Downloads\\c_1925.pdf\",\n",
    "    r\"C:\\Users\\joaqu\\Downloads\\constitucion_chile.pdf\"\n",
    "]\n",
    "\n",
    "# Validar número de documentos y páginas\n",
    "import PyPDF2\n",
    "\n",
    "assert len(doc_paths) >= 2, \"Deben adjuntar un mínimo de 2 documentos\"\n",
    "\n",
    "# Calcular el total de páginas\n",
    "total_paginas = sum(len(PyPDF2.PdfReader(open(doc, \"rb\")).pages) for doc in doc_paths)\n",
    "assert total_paginas >= 50, f\"Páginas insuficientes: {total_paginas}\"\n",
    "print(f\"Total de páginas: {total_paginas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r811-P71nizA"
   },
   "source": [
    "#### **2.1.2 Vectorizar Documentos (0.2 puntos)**\n",
    "\n",
    "Vectorice los documentos y almacene sus representaciones de manera acorde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "n-yXAdCSn4JM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow-intel (c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Documento: c_1925.pdf - Fragmentos generados: 76\n",
      "Documento: constitucion_chile.pdf - Fragmentos generados: 303\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f067d86a6c2741b48440d62441f2ef29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de los embeddings: (379, 384)\n",
      "Vectorización completa. Total de fragmentos generados: 379\n",
      "Los vectores y metadatos se han almacenado en 'vector_store.pkl'.\n"
     ]
    }
   ],
   "source": [
    "# Instalar bibliotecas necesarias\n",
    "%pip install --quiet PyPDF2 sentence-transformers faiss-cpu\n",
    "\n",
    "# Importar librerías necesarias\n",
    "import PyPDF2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "\n",
    "# Función para dividir texto en fragmentos manualmente\n",
    "def dividir_texto(text, chunk_size, chunk_overlap):\n",
    "    fragments = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        fragments.append(text[start:end])\n",
    "        start += chunk_size - chunk_overlap\n",
    "    return fragments\n",
    "\n",
    "# Función para cargar y vectorizar documentos\n",
    "def cargar_y_vectorizar_documentos(doc_paths, chunk_size=1000, chunk_overlap=100):\n",
    "    # Diccionario para almacenar los textos de cada documento\n",
    "    doc_texts = {}\n",
    "\n",
    "    # Extraer texto de los documentos PDF\n",
    "    for doc in doc_paths:\n",
    "        with open(doc, \"rb\") as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text = \"\".join(page.extract_text() for page in reader.pages)\n",
    "            doc_texts[doc] = text  # Guardar el texto extraído en el diccionario\n",
    "\n",
    "    # Dividir textos en fragmentos manejables\n",
    "    texts = []  # Lista para almacenar los fragmentos\n",
    "    metadatas = []  # Lista para los metadatos\n",
    "\n",
    "    for doc_name, text in doc_texts.items():\n",
    "        split_text = dividir_texto(text, chunk_size, chunk_overlap)  # Fragmentar el texto\n",
    "        texts.extend(split_text)  # Agregar los fragmentos a la lista\n",
    "        metadatas.extend([{\"source\": doc_name}] * len(split_text))  # Asociar metadatos a cada fragmento\n",
    "        print(f\"Documento: {doc_name} - Fragmentos generados: {len(split_text)}\")\n",
    "\n",
    "    # Usar SentenceTransformers para generar embeddings\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    embeddings = model.encode(texts, show_progress_bar=True)\n",
    "    print(f\"Dimensiones de los embeddings: {embeddings.shape}\")\n",
    "\n",
    "    # Crear índice FAISS basado en similitud L2\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)  # Añadir los embeddings al índice\n",
    "\n",
    "    # Guardar índice FAISS y metadatos\n",
    "    with open(\"vector_store.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\"index\": index, \"texts\": texts, \"metadatas\": metadatas}, f)\n",
    "\n",
    "    return index, len(texts)\n",
    "\n",
    "# Rutas de documentos (asegúrate de haber subido estos archivos)\n",
    "doc_paths = [\"c_1925.pdf\", \"constitucion_chile.pdf\"]  # Cambia según tus archivos\n",
    "\n",
    "# Ejecutar el proceso de vectorización\n",
    "index, total_chunks = cargar_y_vectorizar_documentos(doc_paths)\n",
    "\n",
    "print(f\"Vectorización completa. Total de fragmentos generados: {total_chunks}\")\n",
    "print(\"Los vectores y metadatos se han almacenado en 'vector_store.pkl'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAUkP5zrnyBK"
   },
   "source": [
    "#### **2.1.3 Habilitar RAG (0.3 puntos)**\n",
    "\n",
    "Habilite la solución RAG a través de una *chain* y guárdela en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPIySdDFn99l"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycg5S5i_n-kL"
   },
   "source": [
    "#### **2.1.4 Verificación de respuestas (0.5 puntos)**\n",
    "\n",
    "Genere un listado de 3 tuplas (\"pregunta\", \"respuesta correcta\") y analice la respuesta de su solución para cada una. ¿Su solución RAG entrega las respuestas que esperaba?\n",
    "\n",
    "Ejemplo de tupla:\n",
    "- Pregunta: ¿Quién es el presidente de Chile?\n",
    "- Respuesta correcta: El presidente de Chile es Gabriel Boric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_UiEn1hoZYR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8d5zTMHoUgF"
   },
   "source": [
    "#### **2.1.5 Sensibilidad de Hiperparámetros (0.5 puntos)**\n",
    "\n",
    "Extienda el análisis del punto 2.1.4 analizando cómo cambian las respuestas entregadas cambiando los siguientes hiperparámetros:\n",
    "- `Tamaño del chunk`. (*¿Cómo repercute que los chunks sean mas grandes o chicos?*)\n",
    "- `La cantidad de chunks recuperados`. (*¿Qué pasa si se devuelven muchos/pocos chunks?*)\n",
    "- `El tipo de búsqueda`. (*¿Cómo afecta el tipo de búsqueda a las respuestas de mi RAG?*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UDh_QgeXLGHc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENJiPPM0giX8"
   },
   "source": [
    "### **2.2 Agentes (1.0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/rcqnN2aJCSEAAAAd/secret-agent-man.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Similar a la sección anterior, en esta sección se busca habilitar **Agentes** para obtener información a través de tools y así responder la pregunta del usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V47l7Mjfrk0N"
   },
   "source": [
    "#### **2.2.1 Tool de Tavily (0.2 puntos)**\n",
    "\n",
    "Generar una *tool* que pueda hacer consultas al motor de búsqueda **Tavily**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6SLKwcWr0AG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SonB1A-9rtRq"
   },
   "source": [
    "#### **2.2.2 Tool de Wikipedia (0.2 puntos)**\n",
    "\n",
    "Generar una *tool* que pueda hacer consultas a **Wikipedia**.\n",
    "\n",
    "*Hint: Le puede ser de ayuda el siguiente [link](https://python.langchain.com/v0.1/docs/modules/tools/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehJJpoqsr26-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvUIMdX6r0ne"
   },
   "source": [
    "#### **2.2.3 Crear Agente (0.3 puntos)**\n",
    "\n",
    "Crear un agente que pueda responder preguntas preguntas usando las *tools* antes generadas. Asegúrese que su agente responda en español. Por último, guarde el agente en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pD1_n0wrsDI5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKV0JxK3r-XG"
   },
   "source": [
    "#### **2.2.4 Verificación de respuestas (0.3 puntos)**\n",
    "\n",
    "Pruebe el funcionamiento de su agente y asegúrese que el agente esté ocupando correctamente las tools disponibles. ¿En qué casos el agente debería ocupar la tool de Tavily? ¿En qué casos debería ocupar la tool de Wikipedia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pqo2dsxvywW_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZbDTYiogquv"
   },
   "source": [
    "### **2.3 Multi Agente (1.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/r7QMJLxU4BoAAAAd/this-is-getting-out-of-hand-star-wars.gif\"\n",
    "\" width=\"450\">\n",
    "</p>\n",
    "\n",
    "El objetivo de esta subsección es encapsular las funcionalidades creadas en una solución multiagente con un **supervisor**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-iUfH0WvI6m"
   },
   "source": [
    "#### **2.3.1 Generando Tools (0.5 puntos)**\n",
    "\n",
    "Transforme la solución RAG de la sección 2.1 y el agente de la sección 2.2 a *tools* (una tool por cada uno)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pw1cfTtvv1AZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQYNjT_0vPCg"
   },
   "source": [
    "#### **2.3.2 Agente Supervisor (0.5 puntos)**\n",
    "\n",
    "Habilite un agente que tenga acceso a las tools del punto anterior y pueda responder preguntas relacionadas. Almacene este agente en una variable llamada supervisor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yv2ZY0BAv1RD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea3zWlvyvY7K"
   },
   "source": [
    "#### **2.3.3 Verificación de respuestas (0.25 puntos)**\n",
    "\n",
    "Pruebe el funcionamiento de su agente repitiendo las preguntas realizadas en las secciones 2.1.4 y 2.2.4 y comente sus resultados. ¿Cómo varían las respuestas bajo este enfoque?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_1t0zkgv1qW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qb8bdAmYvgwn"
   },
   "source": [
    "#### **2.3.4 Análisis (0.25 puntos)**\n",
    "\n",
    "¿Qué diferencias tiene este enfoque con la solución *Router* vista en clases? Nombre al menos una ventaja y desventaja."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAUlJxqoLK5r"
   },
   "source": [
    "`escriba su respuesta acá`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JWVSuWiZ8Mj"
   },
   "source": [
    "### **2.4 Memoria (Bonus +0.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/Gs95aiElrscAAAAd/memory-unlocked-ratatouille-critic.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Una de las principales falencias de las soluciones que hemos visto hasta ahora es que nuestro chat no responde las interacciones anteriores, por ejemplo:\n",
    "\n",
    "- Pregunta 1: \"Hola! mi nombre es Sebastián\"\n",
    "  - Respuesta esperada: \"Hola Sebastián! ...\"\n",
    "- Pregunta 2: \"Cual es mi nombre?\"\n",
    "  - Respuesta actual: \"Lo siento pero no conozco tu nombre :(\"\n",
    "  - **Respuesta esperada: \"Tu nombre es Sebastián\"**\n",
    "\n",
    "Para solucionar esto, se les solicita agregar un componente de **memoria** a la solución entregada en el punto 2.3.\n",
    "\n",
    "**Nota: El Bonus es válido <u>sólo para la sección 2 de Large Language Models.</u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6Y7tIPJLPfB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFc3jBT5g0kT"
   },
   "source": [
    "### **2.5 Despliegue (0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/IytHqOp52EsAAAAd/you-get-a-deploy-deploy.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Una vez tengan los puntos anteriores finalizados, toca la etapa de dar a conocer lo que hicimos! Para eso, vamos a desplegar nuestro modelo a través de `gradio`, una librería especializada en el levantamiento rápido de demos basadas en ML.\n",
    "\n",
    "Primero instalamos la librería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8TsvnCPbkIA"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJBztEUovKsF"
   },
   "source": [
    "Luego sólo deben ejecutar el siguiente código e interactuar con la interfaz a través del notebook o del link generado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3KedQSvg1-n"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "\n",
    "def agent_response(message, history):\n",
    "  '''\n",
    "  Función para gradio, recibe mensaje e historial, devuelte la respuesta del chatbot.\n",
    "  '''\n",
    "  # get chatbot response\n",
    "  response = ... # rellenar con la respuesta de su chat\n",
    "\n",
    "  # assert\n",
    "  assert type(response) == str, \"output de route_question debe ser string\"\n",
    "\n",
    "  # \"streaming\" response\n",
    "  for i in range(len(response)):\n",
    "    time.sleep(0.015)\n",
    "    yield response[: i+1]\n",
    "\n",
    "gr.ChatInterface(\n",
    "    agent_response,\n",
    "    type=\"messages\",\n",
    "    title=\"Chatbot MDS7202\", # Pueden cambiar esto si lo desean\n",
    "    description=\"Hola! Soy un chatbot muy útil :)\", # también la descripción\n",
    "    theme=\"soft\",\n",
    "    ).launch(\n",
    "        share=True, # pueden compartir el link a sus amig@s para que interactuen con su chat!\n",
    "        debug = False,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
