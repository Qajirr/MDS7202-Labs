{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b5c0d2440b3e4995a794ded565213150",
        "deepnote_cell_type": "markdown",
        "id": "_Mql1uRoI5v5"
      },
      "source": [
        "<h1><center>Laboratorio 9: Optimización de modelos 💯</center></h1>\n",
        "\n",
        "<center><strong>MDS7202: Laboratorio de Programación Científica para Ciencia de Datos - Primavera 2024</strong></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "bfb94b9656f145ad83e81b75d218cb70",
        "deepnote_cell_type": "markdown",
        "id": "FAPGIlEAI5v8"
      },
      "source": [
        "### **Cuerpo Docente:**\n",
        "\n",
        "- Profesores: Ignacio Meza, Sebastián Tinoco\n",
        "- Auxiliar: Eduardo Moya\n",
        "- Ayudantes: Nicolás Ojeda, Melanie Peña, Valentina Rojas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b1b537fdd27c43909a49d3476ce64d91",
        "deepnote_cell_type": "markdown",
        "id": "8NozgbkZI5v9"
      },
      "source": [
        "### Equipo: **SUPER IMPORTANTE - notebooks sin nombre no serán revisados**\n",
        "\n",
        "- Nombre de alumno 1: Joaquín De Groote\n",
        "- Nombre de alumno 2: Vicente Pinochet R."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Link de repositorio de GitHub:** [Insertar Enlace](https://github.com/Qajirr/MDS7202-Labs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b7dbdd30ab544cb8a8afe00648a586ae",
        "deepnote_cell_type": "markdown",
        "id": "vHU9DI6wI5v9"
      },
      "source": [
        "### Temas a tratar\n",
        "\n",
        "- Predicción de demanda usando `xgboost`\n",
        "- Búsqueda del modelo óptimo de clasificación usando `optuna`\n",
        "- Uso de pipelines.\n",
        "\n",
        "### Reglas:\n",
        "\n",
        "- **Grupos de 2 personas**\n",
        "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente serán respondidos por este medio.\n",
        "- Prohibidas las copias.\n",
        "- Pueden usar cualquer matrial del curso que estimen conveniente.\n",
        "- Código que no se pueda ejecutar, no será revisado.\n",
        "\n",
        "### Objetivos principales del laboratorio\n",
        "\n",
        "- Optimizar modelos usando `optuna`\n",
        "- Recurrir a técnicas de *prunning*\n",
        "- Forzar el aprendizaje de relaciones entre variables mediante *constraints*\n",
        "- Fijar un pipeline con un modelo base que luego se irá optimizando.\n",
        "\n",
        "El laboratorio deberá ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al máximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante más eficientes que los iteradores nativos sobre DataFrames."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "f1c73babb7f74af588a4fa6ae14829e0",
        "deepnote_cell_type": "markdown",
        "id": "U_-sNOuOI5v9"
      },
      "source": [
        "# Importamos librerias útiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cell_id": "51afe4d2df42442b9e5402ffece60ead",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 4957,
        "execution_start": 1699544354044,
        "id": "ekHbM85NI5v9",
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "!pip install -qq xgboost optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6hJXpLCSspz"
      },
      "source": [
        "# El emprendimiento de Fiu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "44d227389a734ac59189c5e0005bc68a",
        "deepnote_cell_type": "markdown",
        "id": "b0bDalAOI5v-"
      },
      "source": [
        "Tras liderar de manera exitosa la implementación de un proyecto de ciencia de datos para caracterizar los datos generados en Santiago 2023, el misterioso corpóreo **Fiu** se anima y decide levantar su propio negocio de consultoría en machine learning. Tras varias e intensas negociaciones, Fiu logra encontrar su *primera chamba*: predecir la demanda (cantidad de venta) de una famosa productora de bebidas de calibre mundial. Al ver el gran potencial y talento que usted ha demostrado en el campo de la ciencia de datos, Fiu lo contrata como data scientist para que forme parte de su nuevo emprendimiento.\n",
        "\n",
        "Para este laboratorio deben trabajar con los datos `sales.csv` subidos a u-cursos, el cual contiene una muestra de ventas de la empresa para diferentes productos en un determinado tiempo.\n",
        "\n",
        "Para comenzar, cargue el dataset señalado y visualice a través de un `.head` los atributos que posee el dataset.\n",
        "\n",
        "<i><p align=\"center\">Fiu siendo felicitado por su excelente desempeño en el proyecto de caracterización de datos</p></i>\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media-front.elmostrador.cl/2023/09/A_UNO_1506411_2440e.jpg\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "2f9c82d204b14515ad27ae07e0b77702",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 92,
        "execution_start": 1699544359006,
        "id": "QvMPOqHuI5v-",
        "outputId": "659e7a12-d74d-45d6-d3c2-33a6cd338585",
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "df = pd.read_csv('sales.csv')\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b50db6f2cb804932ae3f9e5748a6ea61",
        "deepnote_cell_type": "markdown",
        "id": "pk4ru76pI5v_"
      },
      "source": [
        "## 1 Generando un Baseline (5 puntos)\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media.tenor.com/O-lan6TkadUAAAAC/what-i-wnna-do-after-a-baseline.gif\">\n",
        "</p>\n",
        "\n",
        "Antes de entrenar un algoritmo, usted recuerda los apuntes de su magíster en ciencia de datos y recuerda que debe seguir una serie de *buenas prácticas* para entrenar correcta y debidamente su modelo. Después de un par de vueltas, llega a las siguientes tareas:\n",
        "\n",
        "1. Separe los datos en conjuntos de train (70%), validation (20%) y test (10%). Fije una semilla para controlar la aleatoriedad. [0.5 puntos]\n",
        "2. Implemente un `FunctionTransformer` para extraer el día, mes y año de la variable `date`. Guarde estas variables en el formato categorical de pandas. [1 punto]\n",
        "3. Implemente un `ColumnTransformer` para procesar de manera adecuada los datos numéricos y categóricos. Use `OneHotEncoder` para las variables categóricas. `Nota:` Utilice el método `.set_output(transform='pandas')` para obtener un DataFrame como salida del `ColumnTransformer` [1 punto]\n",
        "4. Guarde los pasos anteriores en un `Pipeline`, dejando como último paso el regresor `DummyRegressor` para generar predicciones en base a promedios. [0.5 punto]\n",
        "5. Entrene el pipeline anterior y reporte la métrica `mean_absolute_error` sobre los datos de validación. ¿Cómo se interpreta esta métrica para el contexto del negocio? [0.5 puntos]\n",
        "6. Finalmente, vuelva a entrenar el `Pipeline` pero esta vez usando `XGBRegressor` como modelo **utilizando los parámetros por default**. ¿Cómo cambia el MAE al implementar este algoritmo? ¿Es mejor o peor que el `DummyRegressor`? [1 punto]\n",
        "7. Guarde ambos modelos en un archivo .pkl (uno cada uno) [0.5 puntos]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn import set_config\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from xgboost import XGBRegressor\n",
        "import pickle\n",
        "set_config(transform_output=\"pandas\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inserte su código acá\n",
        "# 1. Separar los datos\n",
        "train_data, temp_data = train_test_split(df, test_size=0.3, random_state=99)\n",
        "val_data, test_data = train_test_split(temp_data, test_size=0.33, random_state=99)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definir la función para extraer día, mes y año\n",
        "def extract_date_features(df):\n",
        "    df['date'] = pd.to_datetime(df['date'], dayfirst=True)\n",
        "    df['day'] = df['date'].dt.day\n",
        "    df['month'] = df['date'].dt.month\n",
        "    df['year'] = df['date'].dt.year\n",
        "    df['day'] = df['day'].astype('category')\n",
        "    df['month'] = df['month'].astype('category')\n",
        "    df['year'] = df['year'].astype('category')\n",
        "    return df.drop(columns='date')\n",
        "\n",
        "# 2. Crear el transformer\n",
        "date_transformer = FunctionTransformer(extract_date_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Seleccionar las columnas categóricas y numéricas\n",
        "categorical_columns = ['city', 'shop', 'day', 'month', 'year', 'brand',\t'container', 'capacity']\n",
        "numeric_columns = ['id', 'lat',\t'long', 'pop', 'price']\n",
        "\n",
        "# 3. Definir el transformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_columns),\n",
        "        ('cat', OneHotEncoder(sparse_output=False), categorical_columns)\n",
        "    ]\n",
        ")\n",
        "# Configurar salida en formato DataFrame\n",
        "preprocessor.set_output(transform=\"pandas\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Crear el pipeline con el preprocesador y el DummyRegressor\n",
        "pipeline_dummy = Pipeline(steps=[\n",
        "    ('date to categorical', date_transformer),\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', DummyRegressor(strategy='mean'))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Entrenar el modelo\n",
        "X_train = train_data.drop(columns='quantity')\n",
        "y_train = train_data['quantity']\n",
        "pipeline_dummy.fit(X_train, y_train)\n",
        "\n",
        "X_val = val_data.drop(columns='quantity')\n",
        "y_val = val_data['quantity']\n",
        "\n",
        "# Predicciones\n",
        "y_pred_dummy = pipeline_dummy.predict(X_val)\n",
        "\n",
        "# Calcular MAE\n",
        "mae_dummy = mean_absolute_error(y_val, y_pred_dummy)\n",
        "print(f\"MAE del DummyRegressor: {mae_dummy:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La métrica MAE mide la magnitud promedio de los errores en un conjunto de predicciones. En este caso, representa el error promedio en las predicciones de la demanda de ventas de la productora. Un MAE más bajo indica un mejor desempeño.\n",
        "\n",
        "MAE DummyRegressor= 13735.82\n",
        "- Aproximadamente el valor promedio de las ventas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Crear el pipeline con XGBRegressor\n",
        "pipeline_xgb = Pipeline(steps=[\n",
        "    ('date to categorical', date_transformer),\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', XGBRegressor())\n",
        "])\n",
        "\n",
        "# Entrenar el modelo\n",
        "pipeline_xgb.fit(X_train, y_train)\n",
        "\n",
        "# Predicciones\n",
        "y_pred_xgb = pipeline_xgb.predict(X_val)\n",
        "\n",
        "# Calcular MAE\n",
        "mae_xgb = mean_absolute_error(y_val, y_pred_xgb)\n",
        "print(f\"MAE del XGBRegressor: {mae_xgb:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MAE XGBRegressor= 2595.22\n",
        "-  Mejora respecto al Dummy ya que el modelo es capaz de capturar patrones más complejos.\n",
        "-  Es considerablemente un mejor modelo al anterior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cell_id": "1482c992d9494e5582b23dbd3431dbfd",
        "deepnote_cell_type": "code",
        "id": "sfnN7HubI5v_"
      },
      "outputs": [],
      "source": [
        "# 7. Guardar el DummyRegressor\n",
        "with open('dummy_regressor.pkl', 'wb') as f:\n",
        "    pickle.dump(pipeline_dummy, f)\n",
        "\n",
        "# Guardar el XGBRegressor\n",
        "with open('xgb_regressor.pkl', 'wb') as f:\n",
        "    pickle.dump(pipeline_xgb, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "7e17e46063774ec28226fe300d42ffe0",
        "deepnote_cell_type": "markdown",
        "id": "wnyMINdKI5v_"
      },
      "source": [
        "## 2. Forzando relaciones entre parámetros con XGBoost (10 puntos)\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://64.media.tumblr.com/14cc45f9610a6ee341a45fd0d68f4dde/20d11b36022bca7b-bf/s640x960/67ab1db12ff73a530f649ac455c000945d99c0d6.gif\">\n",
        "</p>\n",
        "\n",
        "Un colega aficionado a la economía le *sopla* que la demanda guarda una relación inversa con el precio del producto. Motivado para impresionar al querido corpóreo, se propone hacer uso de esta información para mejorar su modelo realizando las siguientes tareas:\n",
        "\n",
        "1. Vuelva a entrenar el `Pipeline` con `XGBRegressor`, pero esta vez forzando una relación monótona negativa entre el precio y la cantidad. Para aplicar esta restricción apóyese en la siguiente <a href = https://xgboost.readthedocs.io/en/stable/tutorials/monotonic.html>documentación</a>. [6 puntos]\n",
        "\n",
        ">Hint 1: Para implementar el constraint se le sugiere hacerlo especificando el nombre de la variable. De ser así, probablemente le sea útil **mantener el formato de pandas** antes del step de entrenamiento.\n",
        "\n",
        ">Hint 2: Puede obtener el nombre de las columnas en el paso anterior al modelo regresor mediante el método `.get_feature_names_out()`\n",
        "\n",
        "2. Luego, vuelva a reportar el `MAE` sobre el conjunto de validación. [1 puntos]\n",
        "\n",
        "3. ¿Cómo cambia el error al incluir esta relación? ¿Tenía razón su amigo? [2 puntos]\n",
        "\n",
        "4. Guarde su modelo en un archivo .pkl [1 punto]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "f469f3b572be434191d2d5c3f11b20d2",
        "deepnote_cell_type": "code",
        "id": "B7tMnkiAI5v_"
      },
      "outputs": [],
      "source": [
        "# Inserte su código acá\n",
        "# Obtener nombres de las columnas transformadas\n",
        "feature_names = pipeline_xgb.named_steps['preprocessor'].get_feature_names_out()\n",
        "price_index = list(feature_names).index('num__price')\n",
        "\n",
        "# Configurar las restricciones monotónicas\n",
        "relacion_precio = (0,) * len(feature_names)  # Inicializamos relación para cada variable en 0\n",
        "relacion_precio = list(relacion_precio)\n",
        "relacion_precio[price_index] = -1  # Forzamos la relación negativa entre precio y cantidad (monótona decreciente)\n",
        "relacion_precio = tuple(relacion_precio)\n",
        "\n",
        "pipeline_xgb_monotonic = Pipeline(steps=[\n",
        "    ('date to categorical', date_transformer),\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', XGBRegressor(monotone_constraints=relacion_precio))\n",
        "])\n",
        "\n",
        "# Entrenar el modelo\n",
        "pipeline_xgb_monotonic.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predicciones con el nuevo modelo\n",
        "y_pred_xgb_monotonic = pipeline_xgb_monotonic.predict(X_val)\n",
        "\n",
        "# Calcular el MAE\n",
        "mae_xgb_monotonic = mean_absolute_error(y_val, y_pred_xgb_monotonic)\n",
        "print(f\"MAE del XGBRegressor con constraints: {mae_xgb_monotonic:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El MAE NO disminuye al incluir la restricción, eso indica que la relación inversa entre precio y demanda era falsa y su inclusión ha empeorado el rendimiento del modelo.\n",
        "\n",
        "MAE del XGBRegressor con constraints: 2660.86 > MAE del XGBRegressor: 2595.22"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar el modelo con restricciones en un archivo .pkl\n",
        "with open('xgb_regressor_monotonic.pkl', 'wb') as f:\n",
        "    pickle.dump(pipeline_xgb_monotonic, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "e59ef80ed20b4de8921f24da74e87374",
        "deepnote_cell_type": "markdown",
        "id": "5D5-tX4dI5v_"
      },
      "source": [
        "## 1.3 Optimización de Hiperparámetros con Optuna (20 puntos)\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media.tenor.com/fmNdyGN4z5kAAAAi/hacking-lucy.gif\">\n",
        "</p>\n",
        "\n",
        "Luego de presentarle sus resultados, Fiu le pregunta si es posible mejorar *aun más* su modelo. En particular, le comenta de la optimización de hiperparámetros con metodologías bayesianas a través del paquete `optuna`. Como usted es un aficionado al entrenamiento de modelos de ML, se propone implementar la descabellada idea de su jefe.\n",
        "\n",
        "A partir de la mejor configuración obtenida en la sección anterior, utilice `optuna` para optimizar sus hiperparámetros. En particular, se pide que su optimización considere lo siguiente:\n",
        "\n",
        "- Fijar una semilla en las instancias necesarias para garantizar la reproducibilidad de resultados\n",
        "- Utilice `TPESampler` como método de muestreo\n",
        "- De `XGBRegressor`, optimice los siguientes hiperparámetros:\n",
        "    - `learning_rate` buscando valores flotantes en el rango (0.001, 0.1)\n",
        "    - `n_estimators` buscando valores enteros en el rango (50, 1000)\n",
        "    - `max_depth` buscando valores enteros en el rango (3, 10)\n",
        "    - `max_leaves` buscando valores enteros en el rango (0, 100)\n",
        "    - `min_child_weight` buscando valores enteros en el rango (1, 5)\n",
        "    - `reg_alpha` buscando valores flotantes en el rango (0, 1)\n",
        "    - `reg_lambda` buscando valores flotantes en el rango (0, 1)\n",
        "- De `OneHotEncoder`, optimice el hiperparámetro `min_frequency` buscando el mejor valor flotante en el rango (0.0, 1.0)\n",
        "\n",
        "Para ello se pide los siguientes pasos:\n",
        "1. Implemente una función `objective()` que permita minimizar el `MAE` en el conjunto de validación. Use el método `.set_user_attr()` para almacenar el mejor pipeline entrenado. [10 puntos]\n",
        "2. Fije el tiempo de entrenamiento a 5 minutos. [1 punto]\n",
        "3. Optimizar el modelo y reportar el número de *trials*, el `MAE` y los mejores hiperparámetros encontrados. ¿Cómo cambian sus resultados con respecto a la sección anterior? ¿A qué se puede deber esto? [3 puntos]\n",
        "4. Explique cada hiperparámetro y su rol en el modelo. ¿Hacen sentido los rangos de optimización indicados? [5 puntos]\n",
        "5. Guardar su modelo en un archivo .pkl [1 punto]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "cell_id": "de5914621cc64cb0b1bacb9ff565a97e",
        "deepnote_cell_type": "code",
        "id": "kMXXi1ckI5v_"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "import optuna\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Definir la función de optimización\n",
        "def objective(trial):\n",
        "    # Espacio de búsqueda de hiperparámetros\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 0.1)\n",
        "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 1000)\n",
        "    max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n",
        "    max_leaves = trial.suggest_int(\"max_leaves\", 0, 100)\n",
        "    min_child_weight = trial.suggest_int(\"min_child_weight\", 1, 5)\n",
        "    reg_alpha = trial.suggest_float(\"reg_alpha\", 0, 1)\n",
        "    reg_lambda = trial.suggest_float(\"reg_lambda\", 0, 1)\n",
        "    min_frequency = trial.suggest_float(\"min_frequency\", 0.0, 1.0)\n",
        "    \n",
        "    # Pipeline con XGBRegressor y optimización de OneHotEncoder\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', StandardScaler(), numeric_columns),\n",
        "            ('cat', OneHotEncoder(sparse_output=False, min_frequency=min_frequency), categorical_columns)\n",
        "        ]\n",
        "    )\n",
        "    \n",
        "    model = XGBRegressor(\n",
        "        learning_rate=learning_rate,\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        max_leaves=max_leaves,\n",
        "        min_child_weight=min_child_weight,\n",
        "        reg_alpha=reg_alpha,\n",
        "        reg_lambda=reg_lambda,\n",
        "        random_state=99\n",
        "    )\n",
        "    \n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('date to categorical', date_transformer),\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', model)\n",
        "    ])\n",
        "    \n",
        "    # Entrenar el pipeline con los datos de entrenamiento\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    \n",
        "    # Predicción en el conjunto de validación\n",
        "    y_pred = pipeline.predict(X_val)\n",
        "    \n",
        "    # Calcular el MAE en el conjunto de validación\n",
        "    mae = mean_absolute_error(y_val, y_pred)\n",
        "    \n",
        "    # Almacenar el pipeline entrenado en el trial si es el mejor hasta ahora\n",
        "    trial.set_user_attr(\"best_pipeline\", pipeline)\n",
        "    \n",
        "    return mae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuración del estudio con Optuna\n",
        "sampler = TPESampler(seed=99)\n",
        "study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
        "\n",
        "# Ejecutar la optimización con un tiempo límite de 5 minutos (300 segundos)\n",
        "study.optimize(objective, timeout=300)\n",
        "\n",
        "# Mostrar los resultados de la optimización\n",
        "best_trial = study.best_trial\n",
        "print(f\"Número de trials: {len(study.trials)}\")\n",
        "print(f\"Mejor MAE: {best_trial.value}\")\n",
        "print(f\"Mejores hiperparámetros: {best_trial.params}\")\n",
        "# Guardar el mejor pipeline encontrado\n",
        "best_pipeline = best_trial.user_attrs[\"best_pipeline\"]\n",
        "\n",
        "# Guardar el modelo en un archivo .pkl\n",
        "with open('best_xgb_pipeline.pkl', 'wb') as f:\n",
        "    pickle.dump(best_pipeline, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "learning_rate: controla la tasa de aprendizaje del modelo. Un valor pequeño hace que el modelo aprenda más lentamente pero con mayor precisión.\n",
        "\n",
        "n_estimators: el número de árboles en el modelo. Un valor mayor permite más árboles, lo que aumenta la capacidad del modelo, pero también puede causar sobreajuste.\n",
        "\n",
        "max_depth: la profundidad máxima de los árboles. Controla la complejidad del modelo. Un valor mayor permite árboles más profundos pero incrementa el riesgo de sobreajuste.\n",
        "\n",
        "max_leaves: número máximo de hojas por árbol. Controla la complejidad de los árboles, permitiendo que se formen más hojas para capturar más patrones.\n",
        "\n",
        "min_child_weight: el peso mínimo de una hoja, lo que afecta el número mínimo de muestras necesarias para dividir un nodo. Valores más altos restringen la división de nodos.\n",
        "\n",
        "reg_alpha: regularización L1 (Lasso), que impone una penalización en las características que no son importantes para forzar la reducción de coeficientes.\n",
        "\n",
        "reg_lambda: regularización L2 (Ridge), que penaliza grandes coeficientes y ayuda a controlar el sobreajuste.\n",
        "\n",
        "min_frequency: en el OneHotEncoder, filtra categorías con frecuencias menores a este valor, lo que puede reducir el ruido causado por categorías poco frecuentes.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5195ccfc37e044ad9453f6eb2754f631",
        "deepnote_cell_type": "markdown",
        "id": "ZglyD_QWI5wA"
      },
      "source": [
        "## 4. Optimización de Hiperparámetros con Optuna y Prunners (17 puntos)\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i.pinimg.com/originals/90/16/f9/9016f919c2259f3d0e8fe465049638a7.gif\">\n",
        "</p>\n",
        "\n",
        "Después de optimizar el rendimiento de su modelo varias veces, Fiu le pregunta si no es posible optimizar el entrenamiento del modelo en sí mismo. Después de leer un par de post de personas de dudosa reputación en la *deepweb*, usted llega a la conclusión que puede cumplir este objetivo mediante la implementación de **Prunning**.\n",
        "\n",
        "Vuelva a optimizar los mismos hiperparámetros que la sección pasada, pero esta vez utilizando **Prunning** en la optimización. En particular, usted debe:\n",
        "\n",
        "- Responder: ¿Qué es prunning? ¿De qué forma debería impactar en el entrenamiento? [2 puntos]\n",
        "- Redefinir la función `objective()` utilizando `optuna.integration.XGBoostPruningCallback` como método de **Prunning** [10 puntos]\n",
        "- Fijar nuevamente el tiempo de entrenamiento a 5 minutos [1 punto]\n",
        "- Reportar el número de *trials*, el `MAE` y los mejores hiperparámetros encontrados. ¿Cómo cambian sus resultados con respecto a la sección anterior? ¿A qué se puede deber esto? [3 puntos]\n",
        "- Guardar su modelo en un archivo .pkl [1 punto]\n",
        "\n",
        "Nota: Si quieren silenciar los prints obtenidos en el prunning, pueden hacerlo mediante el siguiente comando:\n",
        "\n",
        "```\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "```\n",
        "\n",
        "De implementar la opción anterior, pueden especificar `show_progress_bar = True` en el método `optimize` para *más sabor*.\n",
        "\n",
        "Hint: Si quieren especificar parámetros del método .fit() del modelo a través del pipeline, pueden hacerlo por medio de la siguiente sintaxis: `pipeline.fit(stepmodelo__parametro = valor)`\n",
        "\n",
        "Hint2: Este <a href = https://stackoverflow.com/questions/40329576/sklearn-pass-fit-parameters-to-xgboost-in-pipeline>enlace</a> les puede ser de ayuda en su implementación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "cell_id": "eeaa967cd8f6426d8c54f276c17dce79",
        "deepnote_cell_type": "code",
        "id": "sST6Wtj5I5wA"
      },
      "outputs": [],
      "source": [
        "# Inserte su código acá\n",
        "import optuna\n",
        "from optuna.integration import XGBoostPruningCallback\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from optuna.samplers import TPESampler\n",
        "\n",
        "# Redefinir la función objective con pruning\n",
        "# Definir la función objective con el preprocesamiento adecuado\n",
        "def objective(trial):\n",
        "    # Espacio de búsqueda de hiperparámetros\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.05)\n",
        "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 1000)\n",
        "    max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n",
        "    max_leaves = trial.suggest_int(\"max_leaves\", 0, 100)\n",
        "    min_child_weight = trial.suggest_int(\"min_child_weight\", 1, 5)\n",
        "    reg_alpha = trial.suggest_float(\"reg_alpha\", 0, 1)\n",
        "    reg_lambda = trial.suggest_float(\"reg_lambda\", 0, 1)\n",
        "    min_frequency = trial.suggest_float(\"min_frequency\", 0.0, 1.0)\n",
        "    \n",
        "    # Pipeline con preprocesamiento\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', StandardScaler(), numeric_columns),\n",
        "            ('cat', OneHotEncoder(sparse_output=False, min_frequency=min_frequency), categorical_columns)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Crear el modelo con XGBRegressor\n",
        "    model = XGBRegressor(\n",
        "        learning_rate=learning_rate,\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        max_leaves=max_leaves,\n",
        "        min_child_weight=min_child_weight,\n",
        "        reg_alpha=reg_alpha,\n",
        "        reg_lambda=reg_lambda,\n",
        "        random_state=99,\n",
        "        use_label_encoder=False,  # No usar LabelEncoder de XGBoost\n",
        "        enable_categorical=False  # Se asegura que las categóricas sean numéricas ya\n",
        "    )\n",
        "\n",
        "    # Definir el pipeline\n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('date to categorical', date_transformer),  # Para extraer day, month, year\n",
        "        ('preprocessor', preprocessor),  # Preprocesar numéricas y categóricas\n",
        "        ('regressor', model)  # XGBRegressor\n",
        "    ])\n",
        "\n",
        "    # Entrenar el pipeline\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Predicción y cálculo del MAE en el conjunto de validación\n",
        "    y_pred = pipeline.predict(X_val)\n",
        "    mae = mean_absolute_error(y_val, y_pred)\n",
        "\n",
        "    # Almacenar el mejor pipeline si es el mejor hasta ahora\n",
        "    trial.set_user_attr(\"best_pipeline\", pipeline)\n",
        "\n",
        "    return mae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fijar el tiempo a 5 minutos y usar pruning\n",
        "sampler = TPESampler(seed=99)\n",
        "study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
        "\n",
        "# Silenciar los prints y añadir barra de progreso\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "# Ejecutar la optimización con pruning y un tiempo límite de 5 minutos\n",
        "study.optimize(objective, timeout=300, show_progress_bar=True)\n",
        "\n",
        "# Mostrar los resultados de la optimización\n",
        "best_trial = study.best_trial\n",
        "print(f\"Número de trials: {len(study.trials)}\")\n",
        "print(f\"Mejor MAE: {best_trial.value}\")\n",
        "print(f\"Mejores hiperparámetros: {best_trial.params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar el mejor pipeline encontrado\n",
        "best_pipeline = best_trial.user_attrs[\"best_pipeline\"]\n",
        "\n",
        "# Guardar el modelo en un archivo .pkl\n",
        "with open('best_xgb_pipeline_pruning.pkl', 'wb') as f:\n",
        "    pickle.dump(best_pipeline, f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "8a081778cc704fc6bed05393a5419327",
        "deepnote_cell_type": "markdown",
        "id": "ZMiiVaCUI5wA"
      },
      "source": [
        "## 5. Visualizaciones (5 puntos)\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media.tenor.com/F-LgB1xTebEAAAAd/look-at-this-graph-nickelback.gif\">\n",
        "</p>\n",
        "\n",
        "\n",
        "Satisfecho con su trabajo, Fiu le pregunta si es posible generar visualizaciones que permitan entender el entrenamiento de su modelo.\n",
        "\n",
        "A partir del siguiente <a href = https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/005_visualization.html#visualization>enlace</a>, genere las siguientes visualizaciones:\n",
        "\n",
        "1. Gráfico de historial de optimización [1 punto]\n",
        "2. Gráfico de coordenadas paralelas [1 punto]\n",
        "3. Gráfico de importancia de hiperparámetros [1 punto]\n",
        "\n",
        "Comente sus resultados:\n",
        "\n",
        "4. ¿Desde qué *trial* se empiezan a observar mejoras notables en sus resultados? [0.5 puntos]\n",
        "5. ¿Qué tendencias puede observar a partir del gráfico de coordenadas paralelas? [1 punto]\n",
        "6. ¿Cuáles son los hiperparámetros con mayor importancia para la optimización de su modelo? [0.5 puntos]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "0e706dc9a8d946eda7a9eb1f0463c6d7",
        "deepnote_cell_type": "code",
        "id": "xjxAEENAI5wA"
      },
      "outputs": [],
      "source": [
        "# Inserte su código acá\n",
        "from optuna.visualization import plot_optimization_history\n",
        "\n",
        "# Graficar el historial de optimización\n",
        "optuna_history = plot_optimization_history(study)\n",
        "optuna_history.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from optuna.visualization import plot_parallel_coordinate\n",
        "\n",
        "# Graficar coordenadas paralelas\n",
        "parallel_coordinates = plot_parallel_coordinate(study)\n",
        "parallel_coordinates.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from optuna.visualization import plot_param_importances\n",
        "\n",
        "# Graficar la importancia de hiperparámetros\n",
        "param_importance = plot_param_importances(study)\n",
        "param_importance.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. ¿Desde qué trial se empiezan a observar mejoras notables en sus resultados?\n",
        "\n",
        "- Al observar el gráfico de historial de optimización, se puede identificar que desde el trial 7 al 27  los resultados comienzan a mejorar.\n",
        "5. ¿Qué tendencias puede observar a partir del gráfico de coordenadas paralelas?\n",
        "\n",
        "- En el gráfico de coordenadas paralelas podemos identificar se pueden observar que los hiperparámetros como learning_rate, max_depth, y n_estimators parecen tener mayor impacto en los resultados del modelo, ya que son los que muestran mayores variaciones en el valor objetivo en función de sus diferentes configuraciones.\n",
        "\n",
        "6. ¿Cuáles son los hiperparámetros con mayor importancia para la optimización de su modelo?\n",
        "\n",
        "- El gráfico de importancia de hiperparámetros muestra dos parametros como los principales, min_frequency y n_estimators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "ac8a20f445d045a3becf1a518d410a7d",
        "deepnote_cell_type": "markdown",
        "id": "EoW32TA9I5wA"
      },
      "source": [
        "## 6. Síntesis de resultados (3 puntos)\n",
        "\n",
        "Finalmente:\n",
        "\n",
        "1. Genere una tabla resumen del MAE en el conjunto de validación obtenido en los 5 modelos entrenados desde Baseline hasta XGBoost con Constraints, Optuna y Prunning. [1 punto]\n",
        "2. Compare los resultados de la tabla y responda, ¿qué modelo obtiene el mejor rendimiento? [0.5 puntos]\n",
        "3. Cargue el mejor modelo, prediga sobre el conjunto de **test** y reporte su MAE. [0.5 puntos]\n",
        "4. ¿Existen diferencias con respecto a las métricas obtenidas en el conjunto de validación? ¿Porqué puede ocurrir esto? [1 punto]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jq5C6cDnJg9h"
      },
      "outputs": [],
      "source": [
        "# Inserte su código acá\n",
        "# Archivos de modelos\n",
        "modelos_pkl = {\n",
        "    'Dummy Regressor': 'dummy_regressor.pkl',\n",
        "    'XGB Regressor Baseline': 'xgb_regressor.pkl',\n",
        "    'XGB Regressor Monotonic': 'xgb_regressor_monotonic.pkl',\n",
        "    'XGB Optuna': 'best_xgb_pipeline.pkl',\n",
        "    'XGB Optuna con Prunning': 'best_xgb_pipeline_pruning.pkl'\n",
        "}\n",
        "\n",
        "# Diccionario para almacenar los MAEs\n",
        "maes = {}\n",
        "\n",
        "# Cargar y calcular el MAE para cada modelo\n",
        "for nombre, archivo in modelos_pkl.items():\n",
        "    with open(archivo, 'rb') as f:\n",
        "        modelo = pickle.load(f)\n",
        "    predicciones = modelo.predict(X_val)\n",
        "    mae = mean_absolute_error(y_val, predicciones)\n",
        "    maes[nombre] = mae\n",
        "\n",
        "# Crear tabla resumen de MAEs\n",
        "df_mae = pd.DataFrame(list(maes.items()), columns=['Modelo', 'MAE Validación'])\n",
        "print(df_mae)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Basado en el MAE más bajo en el conjunto de validación, el mejor modelo es el XGB Optuna con un MAE de 2150.21. Esto indica que, de los cinco modelos entrenados, la versión optimizada con Optuna sin pruning tiene la mejor capacidad para predecir los valores en el conjunto de validación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtener el mejor modelo con el menor MAE\n",
        "mejor_modelo_nombre = df_mae.loc[df_mae['MAE Validación'].idxmin(), 'Modelo']\n",
        "mejor_modelo_archivo = modelos_pkl[mejor_modelo_nombre]\n",
        "\n",
        "# Cargar el mejor modelo\n",
        "with open(mejor_modelo_archivo, 'rb') as f:\n",
        "    mejor_modelo = pickle.load(f)\n",
        "\n",
        "X_test = test_data.drop(columns='quantity')\n",
        "y_test = test_data['quantity']\n",
        "\n",
        "predicciones_test = mejor_modelo.predict(X_test)\n",
        "\n",
        "# Calcular el MAE en el conjunto de test\n",
        "mae_test = mean_absolute_error(y_test, predicciones_test)\n",
        "print(f\"MAE en el conjunto de test: {mae_test}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sí, existe una diferencia entre el MAE del conjunto de validación (2150.21) y el MAE del conjunto de test (2090.28).\n",
        "\n",
        "Estas diferencias pueden ocurrir por varias razones:\n",
        "\n",
        "- Suerte en el particionamiento: La variabilidad entre los datos de validación y de test puede ser el resultado de cómo se dividieron los datos.\n",
        "\n",
        "- Distribución de los datos: El conjunto de validación y el conjunto de test pueden tener ligeras diferencias en la distribución de las características y los valores objetivo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5c4654d12037494fbd385b4dc6bd1059",
        "deepnote_cell_type": "markdown",
        "id": "E_19tgBEI5wA"
      },
      "source": [
        "# Conclusión\n",
        "Eso ha sido todo para el lab de hoy, recuerden que el laboratorio tiene un plazo de entrega de una semana. Cualquier duda del laboratorio, no duden en contactarnos por mail o U-cursos.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media.tenor.com/8CT1AXElF_cAAAAC/gojo-satoru.gif\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5025de06759f4903a26916c80323bf25",
        "deepnote_cell_type": "markdown",
        "id": "Kq2cFix1I5wA"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown",
        "id": "rAp9UxwiI5wA"
      },
      "source": [
        "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=87110296-876e-426f-b91d-aaf681223468' target=\"_blank\">\n",
        "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
        "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "deepnote": {},
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "f63d38450a6b464c9bb6385cf11db4d9",
    "deepnote_persisted_session": {
      "createdAt": "2023-11-09T16:18:30.203Z"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
