{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b5c0d2440b3e4995a794ded565213150",
        "deepnote_cell_type": "markdown",
        "id": "_Mql1uRoI5v5"
      },
      "source": [
        "<h1><center>Laboratorio 9: Optimizaci칩n de modelos 游눮</center></h1>\n",
        "\n",
        "<center><strong>MDS7202: Laboratorio de Programaci칩n Cient칤fica para Ciencia de Datos - Primavera 2024</strong></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "bfb94b9656f145ad83e81b75d218cb70",
        "deepnote_cell_type": "markdown",
        "id": "FAPGIlEAI5v8"
      },
      "source": [
        "### **Cuerpo Docente:**\n",
        "\n",
        "- Profesores: Ignacio Meza, Sebasti치n Tinoco\n",
        "- Auxiliar: Eduardo Moya\n",
        "- Ayudantes: Nicol치s Ojeda, Melanie Pe침a, Valentina Rojas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b1b537fdd27c43909a49d3476ce64d91",
        "deepnote_cell_type": "markdown",
        "id": "8NozgbkZI5v9"
      },
      "source": [
        "### Equipo: **SUPER IMPORTANTE - notebooks sin nombre no ser치n revisados**\n",
        "\n",
        "- Nombre de alumno 1: Joaqu칤n De Groote\n",
        "- Nombre de alumno 2: Vicente Pinochet R."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Link de repositorio de GitHub:** [Insertar Enlace](https://github.com/Qajirr/MDS7202-Labs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b7dbdd30ab544cb8a8afe00648a586ae",
        "deepnote_cell_type": "markdown",
        "id": "vHU9DI6wI5v9"
      },
      "source": [
        "### Temas a tratar\n",
        "\n",
        "- Predicci칩n de demanda usando `xgboost`\n",
        "- B칰squeda del modelo 칩ptimo de clasificaci칩n usando `optuna`\n",
        "- Uso de pipelines.\n",
        "\n",
        "### Reglas:\n",
        "\n",
        "- **Grupos de 2 personas**\n",
        "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser치n respondidos por este medio.\n",
        "- Prohibidas las copias.\n",
        "- Pueden usar cualquer matrial del curso que estimen conveniente.\n",
        "- C칩digo que no se pueda ejecutar, no ser치 revisado.\n",
        "\n",
        "### Objetivos principales del laboratorio\n",
        "\n",
        "- Optimizar modelos usando `optuna`\n",
        "- Recurrir a t칠cnicas de *prunning*\n",
        "- Forzar el aprendizaje de relaciones entre variables mediante *constraints*\n",
        "- Fijar un pipeline con un modelo base que luego se ir치 optimizando.\n",
        "\n",
        "El laboratorio deber치 ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m치ximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante m치s eficientes que los iteradores nativos sobre DataFrames."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "f1c73babb7f74af588a4fa6ae14829e0",
        "deepnote_cell_type": "markdown",
        "id": "U_-sNOuOI5v9"
      },
      "source": [
        "# Importamos librerias 칰tiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cell_id": "51afe4d2df42442b9e5402ffece60ead",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 4957,
        "execution_start": 1699544354044,
        "id": "ekHbM85NI5v9",
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "!pip install -qq xgboost optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6hJXpLCSspz"
      },
      "source": [
        "# El emprendimiento de Fiu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "44d227389a734ac59189c5e0005bc68a",
        "deepnote_cell_type": "markdown",
        "id": "b0bDalAOI5v-"
      },
      "source": [
        "Tras liderar de manera exitosa la implementaci칩n de un proyecto de ciencia de datos para caracterizar los datos generados en Santiago 2023, el misterioso corp칩reo **Fiu** se anima y decide levantar su propio negocio de consultor칤a en machine learning. Tras varias e intensas negociaciones, Fiu logra encontrar su *primera chamba*: predecir la demanda (cantidad de venta) de una famosa productora de bebidas de calibre mundial. Al ver el gran potencial y talento que usted ha demostrado en el campo de la ciencia de datos, Fiu lo contrata como data scientist para que forme parte de su nuevo emprendimiento.\n",
        "\n",
        "Para este laboratorio deben trabajar con los datos `sales.csv` subidos a u-cursos, el cual contiene una muestra de ventas de la empresa para diferentes productos en un determinado tiempo.\n",
        "\n",
        "Para comenzar, cargue el dataset se침alado y visualice a trav칠s de un `.head` los atributos que posee el dataset.\n",
        "\n",
        "<i><p align=\"center\">Fiu siendo felicitado por su excelente desempe침o en el proyecto de caracterizaci칩n de datos</p></i>\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media-front.elmostrador.cl/2023/09/A_UNO_1506411_2440e.jpg\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "2f9c82d204b14515ad27ae07e0b77702",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 92,
        "execution_start": 1699544359006,
        "id": "QvMPOqHuI5v-",
        "outputId": "659e7a12-d74d-45d6-d3c2-33a6cd338585",
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "df = pd.read_csv('sales.csv')\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b50db6f2cb804932ae3f9e5748a6ea61",
        "deepnote_cell_type": "markdown",
        "id": "pk4ru76pI5v_"
      },
      "source": [
        "## 1 Generando un Baseline (5 puntos)\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media.tenor.com/O-lan6TkadUAAAAC/what-i-wnna-do-after-a-baseline.gif\">\n",
        "</p>\n",
        "\n",
        "Antes de entrenar un algoritmo, usted recuerda los apuntes de su mag칤ster en ciencia de datos y recuerda que debe seguir una serie de *buenas pr치cticas* para entrenar correcta y debidamente su modelo. Despu칠s de un par de vueltas, llega a las siguientes tareas:\n",
        "\n",
        "1. Separe los datos en conjuntos de train (70%), validation (20%) y test (10%). Fije una semilla para controlar la aleatoriedad. [0.5 puntos]\n",
        "2. Implemente un `FunctionTransformer` para extraer el d칤a, mes y a침o de la variable `date`. Guarde estas variables en el formato categorical de pandas. [1 punto]\n",
        "3. Implemente un `ColumnTransformer` para procesar de manera adecuada los datos num칠ricos y categ칩ricos. Use `OneHotEncoder` para las variables categ칩ricas. `Nota:` Utilice el m칠todo `.set_output(transform='pandas')` para obtener un DataFrame como salida del `ColumnTransformer` [1 punto]\n",
        "4. Guarde los pasos anteriores en un `Pipeline`, dejando como 칰ltimo paso el regresor `DummyRegressor` para generar predicciones en base a promedios. [0.5 punto]\n",
        "5. Entrene el pipeline anterior y reporte la m칠trica `mean_absolute_error` sobre los datos de validaci칩n. 쮺칩mo se interpreta esta m칠trica para el contexto del negocio? [0.5 puntos]\n",
        "6. Finalmente, vuelva a entrenar el `Pipeline` pero esta vez usando `XGBRegressor` como modelo **utilizando los par치metros por default**. 쮺칩mo cambia el MAE al implementar este algoritmo? 쮼s mejor o peor que el `DummyRegressor`? [1 punto]\n",
        "7. Guarde ambos modelos en un archivo .pkl (uno cada uno) [0.5 puntos]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn import set_config\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from xgboost import XGBRegressor\n",
        "import pickle\n",
        "set_config(transform_output=\"pandas\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inserte su c칩digo ac치\n",
        "# 1. Separar los datos\n",
        "train_data, temp_data = train_test_split(df, test_size=0.3, random_state=99)\n",
        "val_data, test_data = train_test_split(temp_data, test_size=0.33, random_state=99)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definir la funci칩n para extraer d칤a, mes y a침o\n",
        "def extract_date_features(df):\n",
        "    df['date'] = pd.to_datetime(df['date'], dayfirst=True)\n",
        "    df['day'] = df['date'].dt.day\n",
        "    df['month'] = df['date'].dt.month\n",
        "    df['year'] = df['date'].dt.year\n",
        "    df['day'] = df['day'].astype('category')\n",
        "    df['month'] = df['month'].astype('category')\n",
        "    df['year'] = df['year'].astype('category')\n",
        "    return df.drop(columns='date')\n",
        "\n",
        "# 2. Crear el transformer\n",
        "date_transformer = FunctionTransformer(extract_date_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Seleccionar las columnas categ칩ricas y num칠ricas\n",
        "categorical_columns = ['city', 'shop', 'day', 'month', 'year', 'brand',\t'container', 'capacity']\n",
        "numeric_columns = ['id', 'lat',\t'long', 'pop', 'price']\n",
        "\n",
        "# 3. Definir el transformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_columns),\n",
        "        ('cat', OneHotEncoder(sparse_output=False), categorical_columns)\n",
        "    ]\n",
        ")\n",
        "# Configurar salida en formato DataFrame\n",
        "preprocessor.set_output(transform=\"pandas\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Crear el pipeline con el preprocesador y el DummyRegressor\n",
        "pipeline_dummy = Pipeline(steps=[\n",
        "    ('date to categorical', date_transformer),\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', DummyRegressor(strategy='mean'))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Entrenar el modelo\n",
        "X_train = train_data.drop(columns='quantity')\n",
        "y_train = train_data['quantity']\n",
        "pipeline_dummy.fit(X_train, y_train)\n",
        "\n",
        "X_val = val_data.drop(columns='quantity')\n",
        "y_val = val_data['quantity']\n",
        "\n",
        "# Predicciones\n",
        "y_pred_dummy = pipeline_dummy.predict(X_val)\n",
        "\n",
        "# Calcular MAE\n",
        "mae_dummy = mean_absolute_error(y_val, y_pred_dummy)\n",
        "print(f\"MAE del DummyRegressor: {mae_dummy:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La m칠trica MAE mide la magnitud promedio de los errores en un conjunto de predicciones. En este caso, representa el error promedio en las predicciones de la demanda de ventas de la productora. Un MAE m치s bajo indica un mejor desempe침o.\n",
        "\n",
        "MAE DummyRegressor= 13735.82\n",
        "- Aproximadamente el valor promedio de las ventas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Crear el pipeline con XGBRegressor\n",
        "pipeline_xgb = Pipeline(steps=[\n",
        "    ('date to categorical', date_transformer),\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', XGBRegressor())\n",
        "])\n",
        "\n",
        "# Entrenar el modelo\n",
        "pipeline_xgb.fit(X_train, y_train)\n",
        "\n",
        "# Predicciones\n",
        "y_pred_xgb = pipeline_xgb.predict(X_val)\n",
        "\n",
        "# Calcular MAE\n",
        "mae_xgb = mean_absolute_error(y_val, y_pred_xgb)\n",
        "print(f\"MAE del XGBRegressor: {mae_xgb:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MAE XGBRegressor= 2595.22\n",
        "-  Mejora respecto al Dummy ya que el modelo es capaz de capturar patrones m치s complejos.\n",
        "-  Es considerablemente un mejor modelo al anterior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cell_id": "1482c992d9494e5582b23dbd3431dbfd",
        "deepnote_cell_type": "code",
        "id": "sfnN7HubI5v_"
      },
      "outputs": [],
      "source": [
        "# 7. Guardar el DummyRegressor\n",
        "with open('dummy_regressor.pkl', 'wb') as f:\n",
        "    pickle.dump(pipeline_dummy, f)\n",
        "\n",
        "# Guardar el XGBRegressor\n",
        "with open('xgb_regressor.pkl', 'wb') as f:\n",
        "    pickle.dump(pipeline_xgb, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "7e17e46063774ec28226fe300d42ffe0",
        "deepnote_cell_type": "markdown",
        "id": "wnyMINdKI5v_"
      },
      "source": [
        "## 2. Forzando relaciones entre par치metros con XGBoost (10 puntos)\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://64.media.tumblr.com/14cc45f9610a6ee341a45fd0d68f4dde/20d11b36022bca7b-bf/s640x960/67ab1db12ff73a530f649ac455c000945d99c0d6.gif\">\n",
        "</p>\n",
        "\n",
        "Un colega aficionado a la econom칤a le *sopla* que la demanda guarda una relaci칩n inversa con el precio del producto. Motivado para impresionar al querido corp칩reo, se propone hacer uso de esta informaci칩n para mejorar su modelo realizando las siguientes tareas:\n",
        "\n",
        "1. Vuelva a entrenar el `Pipeline` con `XGBRegressor`, pero esta vez forzando una relaci칩n mon칩tona negativa entre el precio y la cantidad. Para aplicar esta restricci칩n ap칩yese en la siguiente <a href = https://xgboost.readthedocs.io/en/stable/tutorials/monotonic.html>documentaci칩n</a>. [6 puntos]\n",
        "\n",
        ">Hint 1: Para implementar el constraint se le sugiere hacerlo especificando el nombre de la variable. De ser as칤, probablemente le sea 칰til **mantener el formato de pandas** antes del step de entrenamiento.\n",
        "\n",
        ">Hint 2: Puede obtener el nombre de las columnas en el paso anterior al modelo regresor mediante el m칠todo `.get_feature_names_out()`\n",
        "\n",
        "2. Luego, vuelva a reportar el `MAE` sobre el conjunto de validaci칩n. [1 puntos]\n",
        "\n",
        "3. 쮺칩mo cambia el error al incluir esta relaci칩n? 쯊en칤a raz칩n su amigo? [2 puntos]\n",
        "\n",
        "4. Guarde su modelo en un archivo .pkl [1 punto]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "f469f3b572be434191d2d5c3f11b20d2",
        "deepnote_cell_type": "code",
        "id": "B7tMnkiAI5v_"
      },
      "outputs": [],
      "source": [
        "# Inserte su c칩digo ac치\n",
        "# Obtener nombres de las columnas transformadas\n",
        "feature_names = pipeline_xgb.named_steps['preprocessor'].get_feature_names_out()\n",
        "price_index = list(feature_names).index('num__price')\n",
        "\n",
        "# Configurar las restricciones monot칩nicas\n",
        "relacion_precio = (0,) * len(feature_names)  # Inicializamos relaci칩n para cada variable en 0\n",
        "relacion_precio = list(relacion_precio)\n",
        "relacion_precio[price_index] = -1  # Forzamos la relaci칩n negativa entre precio y cantidad (mon칩tona decreciente)\n",
        "relacion_precio = tuple(relacion_precio)\n",
        "\n",
        "pipeline_xgb_monotonic = Pipeline(steps=[\n",
        "    ('date to categorical', date_transformer),\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', XGBRegressor(monotone_constraints=relacion_precio))\n",
        "])\n",
        "\n",
        "# Entrenar el modelo\n",
        "pipeline_xgb_monotonic.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predicciones con el nuevo modelo\n",
        "y_pred_xgb_monotonic = pipeline_xgb_monotonic.predict(X_val)\n",
        "\n",
        "# Calcular el MAE\n",
        "mae_xgb_monotonic = mean_absolute_error(y_val, y_pred_xgb_monotonic)\n",
        "print(f\"MAE del XGBRegressor con constraints: {mae_xgb_monotonic:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El MAE NO disminuye al incluir la restricci칩n, eso indica que la relaci칩n inversa entre precio y demanda era falsa y su inclusi칩n ha empeorado el rendimiento del modelo.\n",
        "\n",
        "MAE del XGBRegressor con constraints: 2660.86 > MAE del XGBRegressor: 2595.22"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar el modelo con restricciones en un archivo .pkl\n",
        "with open('xgb_regressor_monotonic.pkl', 'wb') as f:\n",
        "    pickle.dump(pipeline_xgb_monotonic, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "e59ef80ed20b4de8921f24da74e87374",
        "deepnote_cell_type": "markdown",
        "id": "5D5-tX4dI5v_"
      },
      "source": [
        "## 1.3 Optimizaci칩n de Hiperpar치metros con Optuna (20 puntos)\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media.tenor.com/fmNdyGN4z5kAAAAi/hacking-lucy.gif\">\n",
        "</p>\n",
        "\n",
        "Luego de presentarle sus resultados, Fiu le pregunta si es posible mejorar *aun m치s* su modelo. En particular, le comenta de la optimizaci칩n de hiperpar치metros con metodolog칤as bayesianas a trav칠s del paquete `optuna`. Como usted es un aficionado al entrenamiento de modelos de ML, se propone implementar la descabellada idea de su jefe.\n",
        "\n",
        "A partir de la mejor configuraci칩n obtenida en la secci칩n anterior, utilice `optuna` para optimizar sus hiperpar치metros. En particular, se pide que su optimizaci칩n considere lo siguiente:\n",
        "\n",
        "- Fijar una semilla en las instancias necesarias para garantizar la reproducibilidad de resultados\n",
        "- Utilice `TPESampler` como m칠todo de muestreo\n",
        "- De `XGBRegressor`, optimice los siguientes hiperpar치metros:\n",
        "    - `learning_rate` buscando valores flotantes en el rango (0.001, 0.1)\n",
        "    - `n_estimators` buscando valores enteros en el rango (50, 1000)\n",
        "    - `max_depth` buscando valores enteros en el rango (3, 10)\n",
        "    - `max_leaves` buscando valores enteros en el rango (0, 100)\n",
        "    - `min_child_weight` buscando valores enteros en el rango (1, 5)\n",
        "    - `reg_alpha` buscando valores flotantes en el rango (0, 1)\n",
        "    - `reg_lambda` buscando valores flotantes en el rango (0, 1)\n",
        "- De `OneHotEncoder`, optimice el hiperpar치metro `min_frequency` buscando el mejor valor flotante en el rango (0.0, 1.0)\n",
        "\n",
        "Para ello se pide los siguientes pasos:\n",
        "1. Implemente una funci칩n `objective()` que permita minimizar el `MAE` en el conjunto de validaci칩n. Use el m칠todo `.set_user_attr()` para almacenar el mejor pipeline entrenado. [10 puntos]\n",
        "2. Fije el tiempo de entrenamiento a 5 minutos. [1 punto]\n",
        "3. Optimizar el modelo y reportar el n칰mero de *trials*, el `MAE` y los mejores hiperpar치metros encontrados. 쮺칩mo cambian sus resultados con respecto a la secci칩n anterior? 쮸 qu칠 se puede deber esto? [3 puntos]\n",
        "4. Explique cada hiperpar치metro y su rol en el modelo. 쮿acen sentido los rangos de optimizaci칩n indicados? [5 puntos]\n",
        "5. Guardar su modelo en un archivo .pkl [1 punto]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "cell_id": "de5914621cc64cb0b1bacb9ff565a97e",
        "deepnote_cell_type": "code",
        "id": "kMXXi1ckI5v_"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "import optuna\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Definir la funci칩n de optimizaci칩n\n",
        "def objective(trial):\n",
        "    # Espacio de b칰squeda de hiperpar치metros\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 0.1)\n",
        "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 1000)\n",
        "    max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n",
        "    max_leaves = trial.suggest_int(\"max_leaves\", 0, 100)\n",
        "    min_child_weight = trial.suggest_int(\"min_child_weight\", 1, 5)\n",
        "    reg_alpha = trial.suggest_float(\"reg_alpha\", 0, 1)\n",
        "    reg_lambda = trial.suggest_float(\"reg_lambda\", 0, 1)\n",
        "    min_frequency = trial.suggest_float(\"min_frequency\", 0.0, 1.0)\n",
        "    \n",
        "    # Pipeline con XGBRegressor y optimizaci칩n de OneHotEncoder\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', StandardScaler(), numeric_columns),\n",
        "            ('cat', OneHotEncoder(sparse_output=False, min_frequency=min_frequency), categorical_columns)\n",
        "        ]\n",
        "    )\n",
        "    \n",
        "    model = XGBRegressor(\n",
        "        learning_rate=learning_rate,\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        max_leaves=max_leaves,\n",
        "        min_child_weight=min_child_weight,\n",
        "        reg_alpha=reg_alpha,\n",
        "        reg_lambda=reg_lambda,\n",
        "        random_state=99\n",
        "    )\n",
        "    \n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('date to categorical', date_transformer),\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', model)\n",
        "    ])\n",
        "    \n",
        "    # Entrenar el pipeline con los datos de entrenamiento\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    \n",
        "    # Predicci칩n en el conjunto de validaci칩n\n",
        "    y_pred = pipeline.predict(X_val)\n",
        "    \n",
        "    # Calcular el MAE en el conjunto de validaci칩n\n",
        "    mae = mean_absolute_error(y_val, y_pred)\n",
        "    \n",
        "    # Almacenar el pipeline entrenado en el trial si es el mejor hasta ahora\n",
        "    trial.set_user_attr(\"best_pipeline\", pipeline)\n",
        "    \n",
        "    return mae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuraci칩n del estudio con Optuna\n",
        "sampler = TPESampler(seed=99)\n",
        "study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
        "\n",
        "# Ejecutar la optimizaci칩n con un tiempo l칤mite de 5 minutos (300 segundos)\n",
        "study.optimize(objective, timeout=300)\n",
        "\n",
        "# Mostrar los resultados de la optimizaci칩n\n",
        "best_trial = study.best_trial\n",
        "print(f\"N칰mero de trials: {len(study.trials)}\")\n",
        "print(f\"Mejor MAE: {best_trial.value}\")\n",
        "print(f\"Mejores hiperpar치metros: {best_trial.params}\")\n",
        "# Guardar el mejor pipeline encontrado\n",
        "best_pipeline = best_trial.user_attrs[\"best_pipeline\"]\n",
        "\n",
        "# Guardar el modelo en un archivo .pkl\n",
        "with open('best_xgb_pipeline.pkl', 'wb') as f:\n",
        "    pickle.dump(best_pipeline, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "learning_rate: controla la tasa de aprendizaje del modelo. Un valor peque침o hace que el modelo aprenda m치s lentamente pero con mayor precisi칩n.\n",
        "\n",
        "n_estimators: el n칰mero de 치rboles en el modelo. Un valor mayor permite m치s 치rboles, lo que aumenta la capacidad del modelo, pero tambi칠n puede causar sobreajuste.\n",
        "\n",
        "max_depth: la profundidad m치xima de los 치rboles. Controla la complejidad del modelo. Un valor mayor permite 치rboles m치s profundos pero incrementa el riesgo de sobreajuste.\n",
        "\n",
        "max_leaves: n칰mero m치ximo de hojas por 치rbol. Controla la complejidad de los 치rboles, permitiendo que se formen m치s hojas para capturar m치s patrones.\n",
        "\n",
        "min_child_weight: el peso m칤nimo de una hoja, lo que afecta el n칰mero m칤nimo de muestras necesarias para dividir un nodo. Valores m치s altos restringen la divisi칩n de nodos.\n",
        "\n",
        "reg_alpha: regularizaci칩n L1 (Lasso), que impone una penalizaci칩n en las caracter칤sticas que no son importantes para forzar la reducci칩n de coeficientes.\n",
        "\n",
        "reg_lambda: regularizaci칩n L2 (Ridge), que penaliza grandes coeficientes y ayuda a controlar el sobreajuste.\n",
        "\n",
        "min_frequency: en el OneHotEncoder, filtra categor칤as con frecuencias menores a este valor, lo que puede reducir el ruido causado por categor칤as poco frecuentes.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5195ccfc37e044ad9453f6eb2754f631",
        "deepnote_cell_type": "markdown",
        "id": "ZglyD_QWI5wA"
      },
      "source": [
        "## 4. Optimizaci칩n de Hiperpar치metros con Optuna y Prunners (17 puntos)\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i.pinimg.com/originals/90/16/f9/9016f919c2259f3d0e8fe465049638a7.gif\">\n",
        "</p>\n",
        "\n",
        "Despu칠s de optimizar el rendimiento de su modelo varias veces, Fiu le pregunta si no es posible optimizar el entrenamiento del modelo en s칤 mismo. Despu칠s de leer un par de post de personas de dudosa reputaci칩n en la *deepweb*, usted llega a la conclusi칩n que puede cumplir este objetivo mediante la implementaci칩n de **Prunning**.\n",
        "\n",
        "Vuelva a optimizar los mismos hiperpar치metros que la secci칩n pasada, pero esta vez utilizando **Prunning** en la optimizaci칩n. En particular, usted debe:\n",
        "\n",
        "- Responder: 쯈u칠 es prunning? 쮻e qu칠 forma deber칤a impactar en el entrenamiento? [2 puntos]\n",
        "- Redefinir la funci칩n `objective()` utilizando `optuna.integration.XGBoostPruningCallback` como m칠todo de **Prunning** [10 puntos]\n",
        "- Fijar nuevamente el tiempo de entrenamiento a 5 minutos [1 punto]\n",
        "- Reportar el n칰mero de *trials*, el `MAE` y los mejores hiperpar치metros encontrados. 쮺칩mo cambian sus resultados con respecto a la secci칩n anterior? 쮸 qu칠 se puede deber esto? [3 puntos]\n",
        "- Guardar su modelo en un archivo .pkl [1 punto]\n",
        "\n",
        "Nota: Si quieren silenciar los prints obtenidos en el prunning, pueden hacerlo mediante el siguiente comando:\n",
        "\n",
        "```\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "```\n",
        "\n",
        "De implementar la opci칩n anterior, pueden especificar `show_progress_bar = True` en el m칠todo `optimize` para *m치s sabor*.\n",
        "\n",
        "Hint: Si quieren especificar par치metros del m칠todo .fit() del modelo a trav칠s del pipeline, pueden hacerlo por medio de la siguiente sintaxis: `pipeline.fit(stepmodelo__parametro = valor)`\n",
        "\n",
        "Hint2: Este <a href = https://stackoverflow.com/questions/40329576/sklearn-pass-fit-parameters-to-xgboost-in-pipeline>enlace</a> les puede ser de ayuda en su implementaci칩n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "cell_id": "eeaa967cd8f6426d8c54f276c17dce79",
        "deepnote_cell_type": "code",
        "id": "sST6Wtj5I5wA"
      },
      "outputs": [],
      "source": [
        "# Inserte su c칩digo ac치\n",
        "import optuna\n",
        "from optuna.integration import XGBoostPruningCallback\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from optuna.samplers import TPESampler\n",
        "\n",
        "# Redefinir la funci칩n objective con pruning\n",
        "# Definir la funci칩n objective con el preprocesamiento adecuado\n",
        "def objective(trial):\n",
        "    # Espacio de b칰squeda de hiperpar치metros\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.05)\n",
        "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 1000)\n",
        "    max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n",
        "    max_leaves = trial.suggest_int(\"max_leaves\", 0, 100)\n",
        "    min_child_weight = trial.suggest_int(\"min_child_weight\", 1, 5)\n",
        "    reg_alpha = trial.suggest_float(\"reg_alpha\", 0, 1)\n",
        "    reg_lambda = trial.suggest_float(\"reg_lambda\", 0, 1)\n",
        "    min_frequency = trial.suggest_float(\"min_frequency\", 0.0, 1.0)\n",
        "    \n",
        "    # Pipeline con preprocesamiento\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', StandardScaler(), numeric_columns),\n",
        "            ('cat', OneHotEncoder(sparse_output=False, min_frequency=min_frequency), categorical_columns)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Crear el modelo con XGBRegressor\n",
        "    model = XGBRegressor(\n",
        "        learning_rate=learning_rate,\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        max_leaves=max_leaves,\n",
        "        min_child_weight=min_child_weight,\n",
        "        reg_alpha=reg_alpha,\n",
        "        reg_lambda=reg_lambda,\n",
        "        random_state=99,\n",
        "        use_label_encoder=False,  # No usar LabelEncoder de XGBoost\n",
        "        enable_categorical=False  # Se asegura que las categ칩ricas sean num칠ricas ya\n",
        "    )\n",
        "\n",
        "    # Definir el pipeline\n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('date to categorical', date_transformer),  # Para extraer day, month, year\n",
        "        ('preprocessor', preprocessor),  # Preprocesar num칠ricas y categ칩ricas\n",
        "        ('regressor', model)  # XGBRegressor\n",
        "    ])\n",
        "\n",
        "    # Entrenar el pipeline\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Predicci칩n y c치lculo del MAE en el conjunto de validaci칩n\n",
        "    y_pred = pipeline.predict(X_val)\n",
        "    mae = mean_absolute_error(y_val, y_pred)\n",
        "\n",
        "    # Almacenar el mejor pipeline si es el mejor hasta ahora\n",
        "    trial.set_user_attr(\"best_pipeline\", pipeline)\n",
        "\n",
        "    return mae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fijar el tiempo a 5 minutos y usar pruning\n",
        "sampler = TPESampler(seed=99)\n",
        "study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
        "\n",
        "# Silenciar los prints y a침adir barra de progreso\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "# Ejecutar la optimizaci칩n con pruning y un tiempo l칤mite de 5 minutos\n",
        "study.optimize(objective, timeout=300, show_progress_bar=True)\n",
        "\n",
        "# Mostrar los resultados de la optimizaci칩n\n",
        "best_trial = study.best_trial\n",
        "print(f\"N칰mero de trials: {len(study.trials)}\")\n",
        "print(f\"Mejor MAE: {best_trial.value}\")\n",
        "print(f\"Mejores hiperpar치metros: {best_trial.params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar el mejor pipeline encontrado\n",
        "best_pipeline = best_trial.user_attrs[\"best_pipeline\"]\n",
        "\n",
        "# Guardar el modelo en un archivo .pkl\n",
        "with open('best_xgb_pipeline_pruning.pkl', 'wb') as f:\n",
        "    pickle.dump(best_pipeline, f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "8a081778cc704fc6bed05393a5419327",
        "deepnote_cell_type": "markdown",
        "id": "ZMiiVaCUI5wA"
      },
      "source": [
        "## 5. Visualizaciones (5 puntos)\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media.tenor.com/F-LgB1xTebEAAAAd/look-at-this-graph-nickelback.gif\">\n",
        "</p>\n",
        "\n",
        "\n",
        "Satisfecho con su trabajo, Fiu le pregunta si es posible generar visualizaciones que permitan entender el entrenamiento de su modelo.\n",
        "\n",
        "A partir del siguiente <a href = https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/005_visualization.html#visualization>enlace</a>, genere las siguientes visualizaciones:\n",
        "\n",
        "1. Gr치fico de historial de optimizaci칩n [1 punto]\n",
        "2. Gr치fico de coordenadas paralelas [1 punto]\n",
        "3. Gr치fico de importancia de hiperpar치metros [1 punto]\n",
        "\n",
        "Comente sus resultados:\n",
        "\n",
        "4. 쮻esde qu칠 *trial* se empiezan a observar mejoras notables en sus resultados? [0.5 puntos]\n",
        "5. 쯈u칠 tendencias puede observar a partir del gr치fico de coordenadas paralelas? [1 punto]\n",
        "6. 쮺u치les son los hiperpar치metros con mayor importancia para la optimizaci칩n de su modelo? [0.5 puntos]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "0e706dc9a8d946eda7a9eb1f0463c6d7",
        "deepnote_cell_type": "code",
        "id": "xjxAEENAI5wA"
      },
      "outputs": [],
      "source": [
        "# Inserte su c칩digo ac치\n",
        "from optuna.visualization import plot_optimization_history\n",
        "\n",
        "# Graficar el historial de optimizaci칩n\n",
        "optuna_history = plot_optimization_history(study)\n",
        "optuna_history.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from optuna.visualization import plot_parallel_coordinate\n",
        "\n",
        "# Graficar coordenadas paralelas\n",
        "parallel_coordinates = plot_parallel_coordinate(study)\n",
        "parallel_coordinates.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from optuna.visualization import plot_param_importances\n",
        "\n",
        "# Graficar la importancia de hiperpar치metros\n",
        "param_importance = plot_param_importances(study)\n",
        "param_importance.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. 쮻esde qu칠 trial se empiezan a observar mejoras notables en sus resultados?\n",
        "\n",
        "- Al observar el gr치fico de historial de optimizaci칩n, se puede identificar que desde el trial 7 al 27  los resultados comienzan a mejorar.\n",
        "5. 쯈u칠 tendencias puede observar a partir del gr치fico de coordenadas paralelas?\n",
        "\n",
        "- En el gr치fico de coordenadas paralelas podemos identificar se pueden observar que los hiperpar치metros como learning_rate, max_depth, y n_estimators parecen tener mayor impacto en los resultados del modelo, ya que son los que muestran mayores variaciones en el valor objetivo en funci칩n de sus diferentes configuraciones.\n",
        "\n",
        "6. 쮺u치les son los hiperpar치metros con mayor importancia para la optimizaci칩n de su modelo?\n",
        "\n",
        "- El gr치fico de importancia de hiperpar치metros muestra dos parametros como los principales, min_frequency y n_estimators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "ac8a20f445d045a3becf1a518d410a7d",
        "deepnote_cell_type": "markdown",
        "id": "EoW32TA9I5wA"
      },
      "source": [
        "## 6. S칤ntesis de resultados (3 puntos)\n",
        "\n",
        "Finalmente:\n",
        "\n",
        "1. Genere una tabla resumen del MAE en el conjunto de validaci칩n obtenido en los 5 modelos entrenados desde Baseline hasta XGBoost con Constraints, Optuna y Prunning. [1 punto]\n",
        "2. Compare los resultados de la tabla y responda, 쯤u칠 modelo obtiene el mejor rendimiento? [0.5 puntos]\n",
        "3. Cargue el mejor modelo, prediga sobre el conjunto de **test** y reporte su MAE. [0.5 puntos]\n",
        "4. 쮼xisten diferencias con respecto a las m칠tricas obtenidas en el conjunto de validaci칩n? 쯇orqu칠 puede ocurrir esto? [1 punto]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jq5C6cDnJg9h"
      },
      "outputs": [],
      "source": [
        "# Inserte su c칩digo ac치\n",
        "# Archivos de modelos\n",
        "modelos_pkl = {\n",
        "    'Dummy Regressor': 'dummy_regressor.pkl',\n",
        "    'XGB Regressor Baseline': 'xgb_regressor.pkl',\n",
        "    'XGB Regressor Monotonic': 'xgb_regressor_monotonic.pkl',\n",
        "    'XGB Optuna': 'best_xgb_pipeline.pkl',\n",
        "    'XGB Optuna con Prunning': 'best_xgb_pipeline_pruning.pkl'\n",
        "}\n",
        "\n",
        "# Diccionario para almacenar los MAEs\n",
        "maes = {}\n",
        "\n",
        "# Cargar y calcular el MAE para cada modelo\n",
        "for nombre, archivo in modelos_pkl.items():\n",
        "    with open(archivo, 'rb') as f:\n",
        "        modelo = pickle.load(f)\n",
        "    predicciones = modelo.predict(X_val)\n",
        "    mae = mean_absolute_error(y_val, predicciones)\n",
        "    maes[nombre] = mae\n",
        "\n",
        "# Crear tabla resumen de MAEs\n",
        "df_mae = pd.DataFrame(list(maes.items()), columns=['Modelo', 'MAE Validaci칩n'])\n",
        "print(df_mae)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Basado en el MAE m치s bajo en el conjunto de validaci칩n, el mejor modelo es el XGB Optuna con un MAE de 2150.21. Esto indica que, de los cinco modelos entrenados, la versi칩n optimizada con Optuna sin pruning tiene la mejor capacidad para predecir los valores en el conjunto de validaci칩n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtener el mejor modelo con el menor MAE\n",
        "mejor_modelo_nombre = df_mae.loc[df_mae['MAE Validaci칩n'].idxmin(), 'Modelo']\n",
        "mejor_modelo_archivo = modelos_pkl[mejor_modelo_nombre]\n",
        "\n",
        "# Cargar el mejor modelo\n",
        "with open(mejor_modelo_archivo, 'rb') as f:\n",
        "    mejor_modelo = pickle.load(f)\n",
        "\n",
        "X_test = test_data.drop(columns='quantity')\n",
        "y_test = test_data['quantity']\n",
        "\n",
        "predicciones_test = mejor_modelo.predict(X_test)\n",
        "\n",
        "# Calcular el MAE en el conjunto de test\n",
        "mae_test = mean_absolute_error(y_test, predicciones_test)\n",
        "print(f\"MAE en el conjunto de test: {mae_test}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "S칤, existe una diferencia entre el MAE del conjunto de validaci칩n (2150.21) y el MAE del conjunto de test (2090.28).\n",
        "\n",
        "Estas diferencias pueden ocurrir por varias razones:\n",
        "\n",
        "- Suerte en el particionamiento: La variabilidad entre los datos de validaci칩n y de test puede ser el resultado de c칩mo se dividieron los datos.\n",
        "\n",
        "- Distribuci칩n de los datos: El conjunto de validaci칩n y el conjunto de test pueden tener ligeras diferencias en la distribuci칩n de las caracter칤sticas y los valores objetivo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5c4654d12037494fbd385b4dc6bd1059",
        "deepnote_cell_type": "markdown",
        "id": "E_19tgBEI5wA"
      },
      "source": [
        "# Conclusi칩n\n",
        "Eso ha sido todo para el lab de hoy, recuerden que el laboratorio tiene un plazo de entrega de una semana. Cualquier duda del laboratorio, no duden en contactarnos por mail o U-cursos.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media.tenor.com/8CT1AXElF_cAAAAC/gojo-satoru.gif\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5025de06759f4903a26916c80323bf25",
        "deepnote_cell_type": "markdown",
        "id": "Kq2cFix1I5wA"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown",
        "id": "rAp9UxwiI5wA"
      },
      "source": [
        "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=87110296-876e-426f-b91d-aaf681223468' target=\"_blank\">\n",
        "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
        "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "deepnote": {},
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "f63d38450a6b464c9bb6385cf11db4d9",
    "deepnote_persisted_session": {
      "createdAt": "2023-11-09T16:18:30.203Z"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
